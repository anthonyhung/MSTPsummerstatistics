<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Anthony Hung" />

<meta name="date" content="2019-05-02" />

<title>Power and Power Analysis</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">ISTP Summer JC: Statistical Theory & Methods</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/anthonyhung/MSTPsummerstatistics">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<!-- Add a small amount of space between sections. -->
  <style type="text/css">
    div.section {
      padding-top: 12px;
    }
  </style>

<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Power and Power Analysis</h1>
<h4 class="author"><em>Anthony Hung</em></h4>
<h4 class="date"><em>2019-05-02</em></h4>

</div>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#reviewing-the-concept-of-power">Reviewing the concept of Power</a></li>
<li><a href="#graphical-depiction-of-power">Graphical depiction of power</a><ul>
<li><a href="#h_0-is-true"><span class="math inline">\(H_0\)</span> is True</a></li>
<li><a href="#h_0-is-false"><span class="math inline">\(H_0\)</span> is False</a></li>
</ul></li>
<li><a href="#factors-that-affect-power">5 factors that affect power:</a><ul>
<li><a href="#introducing-a-scenario-testing-for-difference-in-bird-tail-feather-number">Introducing a scenario: Testing for difference in bird tail feather number</a></li>
<li><a href="#factors-that-affect-power-1.-significance-level-alpha">Factors that affect power: 1. significance level (<span class="math inline">\(\alpha\)</span>)</a></li>
<li><a href="#factors-that-affect-power-2.-sample-size-n">Factors that affect power: 2. Sample size (n)</a></li>
<li><a href="#factors-that-affect-power-3.-variance-in-the-repsonse-variable">Factors that affect power: 3. Variance in the repsonse variable</a></li>
<li><a href="#factors-that-affect-power-4.-magnitude-of-the-effect-size">Factors that affect power: 4. Magnitude of the effect size</a></li>
<li><a href="#factors-that-affect-power-5.-type-of-test-one-or-two-tailed">Factors that affect power: 5. Type of test (one or two tailed)</a></li>
</ul></li>
<li><a href="#performing-a-power-analysis">Performing a power analysis</a><ul>
<li><a href="#notes">Notes</a></li>
</ul></li>
<li><a href="#summary">Summary</a></li>
</ul>
</div>

<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span> workflowr <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks"> Checks <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> </a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2019-05-24
</p>
<p>
<strong>Checks:</strong> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> 5 <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> 1
</p>
<p>
<strong>Knit directory:</strong> <code>MSTPsummerstatistics/</code> <span class="glyphicon glyphicon-question-sign" aria-hidden="true" title="This is the local directory in which the code in this file was executed."> </span>
</p>
<p>
This reproducible <a href="http://rmarkdown.rstudio.com">R Markdown</a> analysis was created with <a
  href="https://github.com/jdblischak/workflowr">workflowr</a> (version 1.3.0). The <em>Checks</em> tab describes the reproducibility checks that were applied when the results were created. The <em>Past versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguncommittedchanges"> <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> <strong>R Markdown file:</strong> uncommitted changes </a>
</p>
</div>
<div id="strongRMarkdownfilestronguncommittedchanges" class="panel-collapse collapse">
<div class="panel-body">
<p>The R Markdown file has unstaged changes. To know which version of the R Markdown file created these results, you’ll want to first commit it to the Git repo. If you’re still working on the analysis, you can ignore this warning. When you’re finished, you can run <code>wflow_publish</code> to commit the R Markdown file and build the HTML.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongEnvironmentstrongempty"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Environment:</strong> empty </a>
</p>
</div>
<div id="strongEnvironmentstrongempty" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSeedstrongcodesetseed20180927code"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Seed:</strong> <code>set.seed(20180927)</code> </a>
</p>
</div>
<div id="strongSeedstrongcodesetseed20180927code" class="panel-collapse collapse">
<div class="panel-body">
<p>The command <code>set.seed(20180927)</code> was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSessioninformationstrongrecorded"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Session information:</strong> recorded </a>
</p>
</div>
<div id="strongSessioninformationstrongrecorded" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Recording the operating system, R version, and package versions is critical for reproducibility.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongCachestrongnone"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Cache:</strong> none </a>
</p>
</div>
<div id="strongCachestrongnone" class="panel-collapse collapse">
<div class="panel-body">
<p>Nice! There were no cached chunks for this analysis, so you can be confident that you successfully produced the results during this run.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsgithubcomanthonyhungMSTPsummerstatisticstree4e210d641f8ab305d79d5c884528be2e26f028f5targetblank4e210d6a"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Repository version:</strong> <a href="https://github.com/anthonyhung/MSTPsummerstatistics/tree/4e210d641f8ab305d79d5c884528be2e26f028f5" target="_blank">4e210d6</a> </a>
</p>
</div>
<div id="strongRepositoryversionstrongahrefhttpsgithubcomanthonyhungMSTPsummerstatisticstree4e210d641f8ab305d79d5c884528be2e26f028f5targetblank4e210d6a" class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility. The version displayed above was the version of the Git repository at the time these results were generated. <br><br> Note that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use <code>wflow_publish</code> or <code>wflow_git_commit</code>). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    .DS_Store
    Ignored:    .Rhistory
    Ignored:    .Rproj.user/
    Ignored:    analysis/.RData
    Ignored:    analysis/.Rhistory

Untracked files:
    Untracked:  docs/figure/powerAnalyses.Rmd/

Unstaged changes:
    Modified:   analysis/Bayes.Rmd
    Modified:   analysis/CLT.Rmd
    Modified:   analysis/index.Rmd
    Modified:   analysis/powerAnalyses.Rmd
    Modified:   analysis/syllabus.Rmd

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the R Markdown and HTML files. If you’ve configured a remote Git repository (see <code>?wflow_git_remote</code>), click on the hyperlinks in the table below to view them.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/c4bdfdc7ed061819bbb246828e4af30054e26b39/docs/powerAnalyses.html" target="_blank">c4bdfdc</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-22
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/dd1e4114724e90bc7711b0f563b508f043146cd2/analysis/powerAnalyses.Rmd" target="_blank">dd1e411</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-22
</td>
<td>
before republishing syllabus
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/dd1e4114724e90bc7711b0f563b508f043146cd2/docs/powerAnalyses.html" target="_blank">dd1e411</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-22
</td>
<td>
before republishing syllabus
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/4ce8e8525dac0d35ce6baff65f0d608a139ba63f/docs/powerAnalyses.html" target="_blank">4ce8e85</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-21
</td>
<td>
bandersnatch add
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/096760a626cc23032b925ccc238d64c8c991c8c7/docs/powerAnalyses.html" target="_blank">096760a</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-18
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/da98ae8f6248396dfbe60499fbe402a628903369/docs/powerAnalyses.html" target="_blank">da98ae8</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-17
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/239723ec3a2b2292484af14f0a209ba997b6cc7b/analysis/powerAnalyses.Rmd" target="_blank">239723e</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-08
</td>
<td>
Update learning objectives
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/2ec7944f6cbaafe7eae338dc3e788fd1ee8b5a25/docs/powerAnalyses.html" target="_blank">2ec7944</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-06
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/d45dca418110b00b669ce0fc53b3824986a0e826/docs/powerAnalyses.html" target="_blank">d45dca4</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-06
</td>
<td>
Republish
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/ee754863c0a68a5f471f3f8d253974ec9c5dba8f/analysis/powerAnalyses.Rmd" target="_blank">ee75486</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-04
</td>
<td>
Build site.
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<style>
hide {
  background-color: #d6d6d6;
  color: #d6d6d6;
}
hide:hover {
  background-color: white;
  color: black;
}
</style>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Before going to the lab to carry out any type of full-scale experiment, it is important to determine how many samples and replicates you will need to include in the experiment to best answer the question you would like to answer. Power analyses allow researchers to determine the smallest <strong>sample size</strong> required to detect the <strong>effect size</strong> of a given comparison at a given <strong>significance level</strong>.</p>
<p>Performing a power analysis before carrying out an experiment has many benefits, among them including:</p>
<ul>
<li><p>Avoiding wasting reagents, animals, or precious samples through an improperly designed experiment that includes more replicates or larger sample sizes than was required.</p></li>
<li><p>Avoiding performing an invalid study that does not have sufficient power to detect a difference of interest.</p></li>
<li><p>Remaining ethical in our conduct of science through avoiding p-hacking by predetermining the number of replicates to perform or number of samples to collect.</p></li>
</ul>
<p>Performing a power analysis after running an experiment is also useful, particularly in the case of a negative result. A question to motivate why it is useful to perform power analyses even after a study is complete, you can ask yourself: “if I performed an experiment and did not detect a statistically significant result, does it necessarily mean that the null hypothesis you were testing is true”?</p>
<p>Our objectives today are to review the concept of power, discuss what a power analysis is, and different ways to carry out a power analysis.</p>
<p>Today, we will first review power as a concept. Then, we will discuss some parameters that need to be kept in mind when trying to estimate power. Finally, we will walk through the steps of performing power analyses and the list of assumptions need to make in order to perform one.</p>
</div>
<div id="reviewing-the-concept-of-power" class="section level2">
<h2>Reviewing the concept of Power</h2>
<p>Recall that there are four possible scenarios when performing a hypothesis test on a null hypothesis. We have previously discussed in some detail the concept of Type 1 and Type 2 errors, which will occur with some probability in any type of test that you will perform.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\(H_0\)</span> is True</th>
<th><span class="math inline">\(H_0\)</span> is False</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>reject <span class="math inline">\(H_0\)</span></td>
<td>P(Type 1 error) = <span class="math inline">\(\alpha\)</span></td>
<td>P(True Positive) = <strong>Power</strong> = <span class="math inline">\(1- \beta\)</span></td>
</tr>
<tr class="even">
<td>fail to reject <span class="math inline">\(H_0\)</span></td>
<td>P(True Negative) = <span class="math inline">\(1-\alpha\)</span></td>
<td>P(Type 2 error) = <span class="math inline">\(\beta\)</span></td>
</tr>
</tbody>
</table>
<p>Power can be thought of as the probability of rejecting the null hypothesis given that the null hypothesis is false (the probability of correctly rejecting the null hypothesis). It is also 1-<span class="math inline">\(\beta\)</span>, the probability of making a type 2 error. Since power is a probability, it takes on values between 0 and 1.</p>
</div>
<div id="graphical-depiction-of-power" class="section level2">
<h2>Graphical depiction of power</h2>
<p>What does it mean that power is the probability of correctly rejecting the null hypothesis? In addition to visualizing it in a table, we can graphically depict what we mean by <span class="math inline">\(\alpha\)</span>, power, and <span class="math inline">\(\beta\)</span>.</p>
<p>Suppose we have two competing hypotheses to consider: a null hypothesis that the mean surface area of chips in a bag is 10 <span class="math inline">\(cm^2\)</span> and an alternative hypothesis that the mean surface area of the chips in the bag is 15 <span class="math inline">\(cm^2\)</span>. You take some samples from the bag and would like to determine which hypothesis better fits the data. Due to sampling error, the null and alternative sampling distributions will have some spread around their mean values. Here we plot the null sampling distribution in blue and the alternative sampling distribution in red:</p>
<pre class="r"><code>library(tidyverse)
library(ggplot2)
library(cowplot)
library(gridExtra)
power_visualization &lt;- ggplot(data = data.frame(x = c(0, 25)), aes(x)) + 
  stat_function(fun = dnorm, args = list(mean = 15, sd = 2), color = &quot;red&quot;, linetype = &quot;solid&quot;) +
  stat_function(fun = dnorm, args = list(mean = 10, sd = 2), color = &quot;blue&quot;, linetype = &quot;solid&quot;)
power_visualization</code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-1-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We would like to perform a one-tailed T-test to determine if there is sufficient evidence in our data to support rejecting the null hypothesis in favor of the alternative. Let us use an <span class="math inline">\(\alpha = 0.05\)</span> and plot the significance threshold on the null distribution:</p>
<pre class="r"><code>power_visualization + geom_vline(xintercept=1.645*2+10, linetype = &quot;dashed&quot;, color = &quot;black&quot;)</code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-2-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Observed means to the right of this threshold would be declared as significant. Now, there are two possible realities: either the null hypothesis (<span class="math inline">\(H_0\)</span>) is indeed True, or <span class="math inline">\(H_0\)</span> is False (see the table above). Let’s first transport ourselves to the reality where the null hypothesis is True.</p>
<div id="h_0-is-true" class="section level3">
<h3><span class="math inline">\(H_0\)</span> is True</h3>
<p>If <span class="math inline">\(H_0\)</span> is true, then looking at the table we created earlier we know that there are two possible results of our hypothesis test. If the results of the test are that we reject <span class="math inline">\(H_0\)</span>, then we have committed a type 1 error (false positive). If the results of the test are that we do not reject <span class="math inline">\(H_0\)</span>, then we have a true negative result.</p>
<p>Since we exist in the reality where the null hypothesis is true, we should only focus on the blue curve (the null sampling distribution).</p>
<pre class="r"><code>H0true &lt;- ggplot(data = data.frame(x = c(0, 25)), aes(x)) + 
  stat_function(fun = dnorm, args = list(mean = 15, sd = 2), color = &quot;red&quot;, linetype = &quot;solid&quot;, alpha = &quot;0.2&quot;)+
  stat_function(fun = dnorm, args = list(mean = 10, sd = 2), color = &quot;blue&quot;, linetype = &quot;solid&quot;) +
  
  stat_function(fun = dnorm, args = list(mean = 10, sd = 2), color = &quot;blue&quot;, linetype = &quot;solid&quot;, xlim = c(0,1.645*2+10), geom = &quot;area&quot;, fill = &quot;yellow&quot;, alpha = 0.5) +
  stat_function(fun = dnorm, args = list(mean = 10, sd = 2), color = &quot;blue&quot;, linetype = &quot;solid&quot;, xlim = c(1.645*2+10, 25), geom = &quot;area&quot;, fill = &quot;green&quot;, alpha = 0.5) +
  
  geom_vline(xintercept=1.645*2+10, linetype = &quot;dashed&quot;, color = &quot;black&quot;)
H0true</code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The area under the null distribution curve to the right of the significance threshold in green is the probability of falsely rejecting the null hypothesis given that the null hypothesis is true (<span class="math inline">\(\alpha\)</span>). Therefore, the area under the curve to the right of the black line is 5% of the total area under the blue curve. Since the total area under the blue curve must add up to 100%, the remaining area to the left of the black line in yellow is 95%, or the probability of a true negative given the null hypothesis is true.</p>
<p>Now, let’s transport ourselves instead to the world where the null hypothesis is False and the alternative hypothesis is true.</p>
</div>
<div id="h_0-is-false" class="section level3">
<h3><span class="math inline">\(H_0\)</span> is False</h3>
<p>If <span class="math inline">\(H_0\)</span> is False, then looking at the table we created earlier we know that there are two possible results of our hypothesis test. If the results of the test are that we reject <span class="math inline">\(H_0\)</span>, then we have a true positive result. If the results of the test are that we do not reject <span class="math inline">\(H_0\)</span>, then we have a type 2 error (False negative).</p>
<p>Since we exist in the reality where the null hypothesis is false, we should only focus on the red curve (the alternative sampling distribution).</p>
<pre class="r"><code>H0false &lt;- ggplot(data = data.frame(x = c(0, 25)), aes(x)) + 
  stat_function(fun = dnorm, args = list(mean = 15, sd = 2), color = &quot;red&quot;, linetype = &quot;solid&quot;)+
  stat_function(fun = dnorm, args = list(mean = 10, sd = 2), color = &quot;blue&quot;, linetype = &quot;solid&quot;, alpha = &quot;0.2&quot;) +

  stat_function(fun = dnorm, args = list(mean = 15, sd = 2), color = &quot;blue&quot;, linetype = &quot;solid&quot;, xlim = c(0,1.645*2+10), geom = &quot;area&quot;, fill = &quot;orange&quot;, alpha = 0.5) +
  stat_function(fun = dnorm, args = list(mean = 15, sd = 2), color = &quot;blue&quot;, linetype = &quot;solid&quot;, xlim = c(1.645*2+10, 25), geom = &quot;area&quot;, fill = &quot;purple&quot;, alpha = 0.5) +
  
  geom_vline(xintercept=1.645*2+10, linetype = &quot;dashed&quot;, color = &quot;black&quot;)
H0false</code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-4-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The area under the alternative distribution curve to the left of the significance threshold in orange is the probability of incorrectly <strong>not</strong> rejecting the null hypothesis given that the null hypothesis is false (False negative, type 2 error). What value should the area under the curve to the left of the black line be? <hide><span class="math inline">\(\beta\)</span></hide> Since the total area under the blue curve must add up to 100%, the remaining area to the left of the black line in purple is <span class="math inline">\(1-\beta\)</span>, or the probability of a true positive given the alternative hypothesis is true. This is also known as the power of the test.</p>
</div>
</div>
<div id="factors-that-affect-power" class="section level2">
<h2>5 factors that affect power:</h2>
<p>There are 5 factors that primarily affect power:</p>
<ol style="list-style-type: decimal">
<li><p>Significance level (<span class="math inline">\(\alpha\)</span>)</p></li>
<li><p>Sample size (n)</p></li>
<li><p>Variance in the response variable</p></li>
<li><p>Magnitude of the effect size</p></li>
<li><p>Type of test (one or two tailed)</p></li>
</ol>
<p>Let’s figure out intuitively why each of these quantities matters when we’re thinking about power.</p>
<div id="introducing-a-scenario-testing-for-difference-in-bird-tail-feather-number" class="section level3">
<h3>Introducing a scenario: Testing for difference in bird tail feather number</h3>
<p>For an example, assume that we are experimenters that are sampling the birds in a forest to see if they have fewer tail feathers than the known species-wide average number of 12.5. Unknown to us, the population mean of tail feathers in this forest is in fact 11, which is less than 12.5. We go out one day and collect 10 birds from the forest and count their number of tail feathers and plot their distribution:</p>
<pre class="r"><code>set.seed(1)
feather_data &lt;-
  data.frame(
    Feathers =  c(rnorm(10, mean = 11, sd = 3))
  )
plot &lt;- ggplot(data = feather_data, aes(x = Feathers)) + geom_density() + xlim(0, 25)
plot</code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>What we can appreciate from the plot is that the distribution of number of feathers of sampled birds is shifted down from 12.5. We can see this through comparing the mean of the distribution (in the black solid line) with 12.5 (in the red dashed line). The spread of our sample distribution is fairly broad, with a standard deviation of 3.</p>
<pre class="r"><code>plot + geom_vline(xintercept=12.5, linetype = &quot;dashed&quot;, color = &quot;red&quot;) + geom_vline(xintercept=mean(feather_data$Feathers), linetype = &quot;solid&quot;, color = &quot;black&quot;) </code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Since we see a difference in the mean of our collected birds from 12.5, we would like to perform a statistical test to determine if this difference is statistically significant. What test should be perform? <hide> T-test for one mean </hide>. Recall that this type of test will be comparing the observed mean to the distribution of values we would expect to see under a null hypothesis. In this case, what this means it that we should find the null distribution for the difference between a sample mean and the species mean and determine what percentile our observed mean falls on this null distribution.</p>
<p>First, start out by plotting the null sampling distribution of feather number (assuming that the variance of feather number in the species known and is equal to the variance in our collected sample). Here it is in red:</p>
<pre class="r"><code>nextplot &lt;-  ggplot(data = data.frame(x = c(-5, 5)), aes(x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 3/sqrt(10)), color = &quot;red&quot;, linetype = &quot;solid&quot;) +
  geom_vline(xintercept=0, linetype = &quot;solid&quot;, color =&quot;red&quot;)
nextplot</code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>What important theorem gives us the expected shape and center of our sampling distribution? <hide>The CLT.</hide> The theorem tells us that our sampling distribution should be centered at 0 (<span class="math inline">\(H_0: \mu_{sampling} - \mu_{species} = 0\)</span>) with standard deviation equal to (<span class="math inline">\(\sigma_{sampling} = \frac{\sigma}{\sqrt{n}}\)</span>).</p>
<p>Recall that in order for a result to be signficant under a one-tailed T-test, the observation should be at a percentile less than or equal to our significance level <span class="math inline">\(\alpha\)</span>. Let’s draw the 5th percentile on the null distribution using a dashed red line to denote our significance level for the test (<span class="math inline">\(\alpha = 0.04\)</span>). Everything to the left of this dotted line would be declared as significant.</p>
<pre class="r"><code>threshold &lt;- (-1.833*3/sqrt(10))
nextplot + geom_vline(xintercept=threshold, linetype = &quot;dashed&quot;, color =&quot;red&quot;) + stat_function(fun = dnorm, args = list(mean = 0, sd = 3/sqrt(10)), color = &quot;red&quot;, linetype = &quot;solid&quot;, xlim = c(-5,threshold), geom = &quot;area&quot;, fill = &quot;red&quot;) </code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Where does our sample mean fall on this null distribution?</p>
<pre class="r"><code>threshold &lt;- (-1.833*3/sqrt(10))
nextplot + geom_vline(xintercept=threshold, linetype = &quot;dashed&quot;, color =&quot;red&quot;) + stat_function(fun = dnorm, args = list(mean = 0, sd = 3/sqrt(10)), color = &quot;red&quot;, linetype = &quot;solid&quot;, xlim = c(-5,threshold), geom = &quot;area&quot;, fill = &quot;red&quot;)  + geom_vline(xintercept = -1.5)</code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Oof! It looks like our sample mean is higher than the threshold for significance for our T-test, meaning we did not detect a signficant difference between our sample and the known species average. How could this be, given that the true population mean was indeed lower than the species mean?</p>
<p>Let us see what effect changing each of the 5 parameters we discussed earlier would have on our ability to correctly detect the true difference in the tail feather number. Note that this is simply an educational example: in a real experiment you obviously would not and should not have the ability to play with parameters of your sample and statistical test after you have collected data.</p>
</div>
<div id="factors-that-affect-power-1.-significance-level-alpha" class="section level3">
<h3>Factors that affect power: 1. significance level (<span class="math inline">\(\alpha\)</span>)</h3>
<p>In order for our T-test to be significant, the observed mean for our sample must be lower than the threshold set by applying our significance level to the null distribution (in red). One way we could imagine that would put our black line (sample mean) to the left of the threshold would be to simply move the red line to the right. This is equivalent to setting a higher value of <span class="math inline">\(\alpha\)</span>. For example, what if in our test we chose to use an <span class="math inline">\(\alpha = 0.10\)</span>?</p>
<pre class="r"><code>threshold &lt;- (-1.383*3/sqrt(10))
nextplot + geom_vline(xintercept=threshold, linetype = &quot;dashed&quot;, color =&quot;red&quot;) + stat_function(fun = dnorm, args = list(mean = 0, sd = 3/sqrt(10)), color = &quot;red&quot;, linetype = &quot;solid&quot;, xlim = c(-5,threshold), geom = &quot;area&quot;, fill = &quot;red&quot;)  + geom_vline(xintercept = -1.5)</code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>At a significance level of 0.10, our test is signficant! We detect a difference between our population’s mean tail number when compared to the species mean tail number at a signficance level of 0.10. Increasing <span class="math inline">\(\alpha\)</span> gives us more power. It’s almost too good to be true. Why might it be inadvisable to simply use a high value of <span class="math inline">\(\alpha\)</span> when performing statistical tests in order to obtain high power? <hide> By increasing <span class="math inline">\(\alpha\)</span>, we not only increase our power, but also our probability of false positives. Recall that <span class="math inline">\(\alpha\)</span> is equal to our probability of obtaining a false positive result.</hide></p>
</div>
<div id="factors-that-affect-power-2.-sample-size-n" class="section level3">
<h3>Factors that affect power: 2. Sample size (n)</h3>
<p>Instead of relaxing our significance threshold, what if we collected more birds in our sample (keeping everything else, including the mean feather number, constant)? For example, let’s assume we collected 50 birds instead of 10. What will this do to the shape of our null sampling distribution? <hide>This will make our null sampling distribution much narrower, since the standard deviation of the null sampling distribution is proportional to <span class="math inline">\(\frac{1}{\sqrt{n}}\)</span></hide>.</p>
<p>Let’s plot the null sampling distribution for a sample size of 50 and draw a dotted line for the threshold for significance under an <span class="math inline">\(\alpha = 0.05\)</span>.</p>
<pre class="r"><code>nextplot &lt;-  ggplot(data = data.frame(x = c(-5, 5)), aes(x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 3/sqrt(50)), color = &quot;red&quot;, linetype = &quot;solid&quot;) +
  geom_vline(xintercept=0, linetype = &quot;solid&quot;, color =&quot;red&quot;)

threshold &lt;- (-1.833*3/sqrt(50))
nextplot + geom_vline(xintercept=threshold, linetype = &quot;dashed&quot;, color =&quot;red&quot;) + stat_function(fun = dnorm, args = list(mean = 0, sd = 3/sqrt(50)), color = &quot;red&quot;, linetype = &quot;solid&quot;, xlim = c(-5,threshold), geom = &quot;area&quot;, fill = &quot;red&quot;) </code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Where does our observed difference in mean tail feather number (11 - 12.5) fall on this null distribution?</p>
<pre class="r"><code>nextplot &lt;-  ggplot(data = data.frame(x = c(-5, 5)), aes(x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 3/sqrt(50)), color = &quot;red&quot;, linetype = &quot;solid&quot;) +
  geom_vline(xintercept=0, linetype = &quot;solid&quot;, color =&quot;red&quot;)

threshold &lt;- (-1.833*3/sqrt(50))
nextplot + geom_vline(xintercept=threshold, linetype = &quot;dashed&quot;, color =&quot;red&quot;) + stat_function(fun = dnorm, args = list(mean = 0, sd = 3/sqrt(50)), color = &quot;red&quot;, linetype = &quot;solid&quot;, xlim = c(-5,threshold), geom = &quot;area&quot;, fill = &quot;red&quot;) + geom_vline(xintercept = -1.5)</code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-12-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>It’s far to the left of the threshold. Therefore, we would conclude that there our observed mean number of tail feathers is significantly different from 12.5. Increasing the sample size gives us increased power to detect a difference, primarily through causing the sampling distribution to be narrower (i.e. we are more confident in our estimate of the population mean using the mean of our sample).</p>
</div>
<div id="factors-that-affect-power-3.-variance-in-the-repsonse-variable" class="section level3">
<h3>Factors that affect power: 3. Variance in the repsonse variable</h3>
<p>In a similar vein, let’s assume that instead of our sample size increasing, the variance in our sample distribution was lower. Recall that our sample distribution had a standard deviation of 3. What is it’s variance? <hide><span class="math inline">\(\sqrt{variance} = standard\_deviation\)</span></hide></p>
<p>If our sample distribution’s sample size was 10 and it’s variance was 1 instead of <span class="math inline">\(\sqrt{3}\)</span>, our null sampling distirbution with an <span class="math inline">\(\alpha = 0.05\)</span> would look like this:</p>
<pre class="r"><code>nextplot &lt;-  ggplot(data = data.frame(x = c(-5, 5)), aes(x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1/sqrt(10)), color = &quot;red&quot;, linetype = &quot;solid&quot;) +
  geom_vline(xintercept=0, linetype = &quot;solid&quot;, color =&quot;red&quot;)

threshold &lt;- (-1.833*1/sqrt(10))
nextplot + geom_vline(xintercept=threshold, linetype = &quot;dashed&quot;, color =&quot;red&quot;) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1/sqrt(10)), color = &quot;red&quot;, linetype = &quot;solid&quot;, xlim = c(-5,threshold), geom = &quot;area&quot;, fill = &quot;red&quot;) </code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-13-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Once again, you can appreciate that the sampling distribution is narrower. Let’s see where our observed mean falls on the distribution:</p>
<pre class="r"><code>nextplot &lt;-  ggplot(data = data.frame(x = c(-5, 5)), aes(x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1/sqrt(10)), color = &quot;red&quot;, linetype = &quot;solid&quot;) +
  geom_vline(xintercept=0, linetype = &quot;solid&quot;, color =&quot;red&quot;)

threshold &lt;- (-1.833*1/sqrt(10))
nextplot + geom_vline(xintercept=threshold, linetype = &quot;dashed&quot;, color =&quot;red&quot;) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1/sqrt(10)), color = &quot;red&quot;, linetype = &quot;solid&quot;, xlim = c(-5,threshold), geom = &quot;area&quot;, fill = &quot;red&quot;) + geom_vline(xintercept = -1.5)</code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Again, the observed mean is significantly lower than 12.5. A reduced variance increases power in the same way that increased sample sizes do, through narrowing the sampling distribution.</p>
</div>
<div id="factors-that-affect-power-4.-magnitude-of-the-effect-size" class="section level3">
<h3>Factors that affect power: 4. Magnitude of the effect size</h3>
<p>Returning to our base example, let’s assume that instead of an average tail feather number of 11, we had found an average of 10 tail feathers per bird in our sample (with the same standard deviation of 3 for our sample). This would mean that our effect size (the difference between our observation mean and the null distribution’s mean) would increase from <span class="math inline">\(11-12.5 = -1.5\)</span> to <span class="math inline">\(10-12.5 = -2.5\)</span>. The effect size is the deviation of an observation from the null hypothesis. Plotting this difference on the null sampling distribution:</p>
<pre class="r"><code>nextplot &lt;-  ggplot(data = data.frame(x = c(-5, 5)), aes(x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 3/sqrt(10)), color = &quot;red&quot;, linetype = &quot;solid&quot;) +
  geom_vline(xintercept=0, linetype = &quot;solid&quot;, color =&quot;red&quot;)
threshold &lt;- (-1.833*3/sqrt(10))
nextplot + geom_vline(xintercept=threshold, linetype = &quot;dashed&quot;, color =&quot;red&quot;) + stat_function(fun = dnorm, args = list(mean = 0, sd = 3/sqrt(10)), color = &quot;red&quot;, linetype = &quot;solid&quot;, xlim = c(-5,threshold), geom = &quot;area&quot;, fill = &quot;red&quot;) + geom_vline(xintercept = -2.5)</code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>we see that the result is significant. Rather than moving the significance threshold, increasing the magnitude of the effect size moves the black line to the left, increasing the power to detect the difference.</p>
</div>
<div id="factors-that-affect-power-5.-type-of-test-one-or-two-tailed" class="section level3">
<h3>Factors that affect power: 5. Type of test (one or two tailed)</h3>
<p>Finally, one easily overlooked aspect of a test that can affect power is whether a test is one or two tailed. Of course, not all statistical tests will have an option of a one or two tailed option. What is an example of a test that is always two-tailed? <hide>Chi-square test</hide> However, in our case a T-test can be either one or two tailed.</p>
<p>Let’s compare the significance thresholds on the null distribution for a one (red) and two (blue) tailed test</p>
<pre class="r"><code>nextplot &lt;-  ggplot(data = data.frame(x = c(-5, 5)), aes(x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 3/sqrt(10)), color = &quot;red&quot;, linetype = &quot;solid&quot;) +
  geom_vline(xintercept=0, linetype = &quot;solid&quot;, color =&quot;red&quot;)

threshold &lt;- (-1.833*3/sqrt(10))
redplot &lt;- nextplot + geom_vline(xintercept=threshold, linetype = &quot;dashed&quot;, color =&quot;red&quot;) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 3/sqrt(10)), color = &quot;red&quot;, linetype = &quot;solid&quot;, xlim = c(-5,threshold), geom = &quot;area&quot;, fill = &quot;red&quot;) 

blueplot &lt;- nextplot + geom_vline(xintercept=(2.262157*3/sqrt(10)), linetype = &quot;dashed&quot;, color =&quot;blue&quot;) + 
  geom_vline(xintercept=(-2.262157*3/sqrt(10)), linetype = &quot;dashed&quot;, color =&quot;blue&quot;) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 3/sqrt(10)), color = &quot;blue&quot;, linetype = &quot;solid&quot;, xlim = c((2.262157*3/sqrt(10)),5), geom = &quot;area&quot;, fill = &quot;blue&quot;) + 
  stat_function(fun = dnorm, args = list(mean = 0, sd = 3/sqrt(10)), color = &quot;blue&quot;, linetype = &quot;solid&quot;, xlim = c(-5,(-2.262157*3/sqrt(10))), geom = &quot;area&quot;, fill = &quot;blue&quot;)

grid.arrange(redplot, blueplot, ncol = 2)</code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Comparing the two, you can see that the threshold for significance for a two-tailed test is more extreme on the left side than for a one-tailed test. This is because for a two tailed test, you would be testing for any type of difference between an observed mean and the null mean (significantly greater or significantly smaller). The shaded portions of the two distributions have the same total area (which is equal to <span class="math inline">\(\alpha\)</span>), which means that there is less shaded area on the left side of the graph (half as much as in a one-tailed test). Therefore, a one-tailed test will have more power to detect a one-directional difference than a two-tailed test. However, one-tailed tests are only appropriate to use in cases where there is an <em>a priori</em> reason to expect that the observed quantity will be less than (or greater than) the null value.</p>
</div>
</div>
<div id="performing-a-power-analysis" class="section level2">
<h2>Performing a power analysis</h2>
<p>Now that we have walked through how many different aspects of a statistical test can affect power, we can move on to discussing how to perform a power analysis. Note that the specific details of a power analysis will differ depending on the specific statistical test you plan to perform.</p>
<p>Two important types of power analyses you can carry out while designing an experiment are <em>a priori</em> analyses and sensitivity analyses.</p>
<ol style="list-style-type: decimal">
<li>A priori: compute sample size required to achieve a desired power, given <span class="math inline">\(\alpha\)</span>, effect size, and type of test.</li>
</ol>
<p>The a priori power analysis tells you what sample size is needed to detect a certain effect size. This is useful when you have a good idea of what range of effect size would be useful or of good clinical utility to detect. For example, if you are comparing the effectiveness of two drugs in stopping hair loss and one is 100 times more expensive than the other, you may not be as interested in determining if one is 0.01% more effective than the other.</p>
<ol start="2" style="list-style-type: decimal">
<li>Sensitivity: compute the effect size that could be detected by a test, given <span class="math inline">\(\alpha\)</span>, power, sample size, and type of test</li>
</ol>
<p>A sensitivity power analysis is used when you as a researcher have no control over the sample size of the study (for example, there is not enough funding to process more than X samples). Instead, this analysis determines what effect size you could detect with the subjects you have. This is referred to as the minimal detectable effect (MDE) of your study.</p>
<p>Let us walk through an example of performing a power analysis to determine a sample size. We will move forward by revisiting our previous experiment of trying to determine the true mean number of tail feathers of birds in a given forest. Suppose again that the known species-wide average number is 12.5 with a standard deviation of 3 and the the population mean of tail feathers in this forest is in fact 11, which is less than 12.5. How many birds must we collect from the forest to have a power of 0.80 to detect the difference using a one-tailed T-test at an <span class="math inline">\(\alpha =0.05\)</span>?</p>
<p>From the statement of the problem, we already know a few of the factors that we need to know in order to perform the power analysis. If we did not have these, we would need to estimate or make assumptions about them before moving forward.</p>
<table>
<thead>
<tr class="header">
<th>Factor</th>
<th>Value</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Significance level</td>
<td>0.05</td>
<td></td>
</tr>
<tr class="even">
<td>Sample size</td>
<td>?</td>
<td></td>
</tr>
<tr class="odd">
<td>Variance</td>
<td><span class="math inline">\(\sqrt{3}\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>Effect size</td>
<td><span class="math inline">\(11-12.5=-1.5\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Type of test</td>
<td>one-tailed</td>
<td></td>
</tr>
<tr class="even">
<td>Power</td>
<td>0.80</td>
<td></td>
</tr>
</tbody>
</table>
<p>First, we start with our significance level of 0.05. Given that we are performing a one-tailed T-test for a single mean, we can figure out the T statistic threshold (“critical value”) for obtaining a significant result. We can do this in R using the command ‘qt(0.05, df = 9)’.</p>
<pre class="r"><code>qt(0.05, df = 9)</code></pre>
<pre><code>[1] -1.833113</code></pre>
<p>Recall that the equation for a T-statistic is given by:</p>
<p><span class="math display">\[T = \frac{\mu_{observed}-\mu_0}{\sqrt{\frac{\sigma^2}{n}}}\]</span></p>
<p>Recall also that the definition of power is the area shaded in purple in this plot</p>
<pre class="r"><code>ggplot(data = data.frame(x = c(0, 25)), aes(x)) + 
  stat_function(fun = dnorm, args = list(mean = 15, sd = 2), color = &quot;red&quot;, linetype = &quot;solid&quot;)+
  stat_function(fun = dnorm, args = list(mean = 10, sd = 2), color = &quot;blue&quot;, linetype = &quot;solid&quot;) +

  stat_function(fun = dnorm, args = list(mean = 15, sd = 2), color = &quot;blue&quot;, linetype = &quot;solid&quot;, xlim = c(1.645*2+10, 25), geom = &quot;area&quot;, fill = &quot;purple&quot;, alpha = 0.5) +
  geom_vline(xintercept=1.645*2+10, linetype = &quot;dashed&quot;, color = &quot;black&quot;) +
  
  geom_vline(xintercept=10, linetype = &quot;dashed&quot;, color = &quot;blue&quot;, alpha = 0.5) +
  geom_vline(xintercept=15, linetype = &quot;dashed&quot;, color = &quot;red&quot;, alpha = 0.5)</code></pre>
<p><img src="figure/powerAnalyses.Rmd/unnamed-chunk-18-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The next step is important but may be difficult to conceptualize. <strong>In order for the purple area to be equal to our power of 0.80, we need the “distance” (in standard error units) between the mean of the alternative distribution and the significance level to be equal to the T statistic corresponding to the 20th percentile.</strong> Why does this make sense? Because the purple shaded area is defined as the area to the right of the significance level in the alternative distribution. The area to the right of the 20th percentile makes up 80% of the total area (0.80). Great! So we know our goal. How do we set our sample size to get to that goal?</p>
<p>To start, we can take advantage of another measurement we have: the effect size. This is the difference between the mean of the null distribution and the alternative distribution. This is a great measuring stick to have because it is set: it will not change depending on the sample size, whereas the standard error of each distribution depends on n, which is the very quantity we are trying to estimate!</p>
<p>In short, we need our effect size (scaled by standard error units) to equal the sum of the distance between the blue dotted line and the black dotted line and the distance between the black dotted line and the red dotted line (in standard error units). The distance between the mean of each distribution and another point scaled by standard error units is another way to phrase a T statistic. In other words:</p>
<p><span class="math display">\[\frac{ES}{\frac{\sigma}{\sqrt{n}}} = T_{1-\alpha} + T_{1-\beta}\]</span></p>
<p><span class="math inline">\(T_{1-\alpha}\)</span> is the T statistic representing the distance between the blue dotted line and the black dotted line scaled in standard error units, while <span class="math inline">\(T_{1-\beta}\)</span> is the T statistic representing the distance between the black dotted line and the red dotted line scaled in standard error units. Rearranging to obtain an equation to solve for n, we get:</p>
<p><span class="math display">\[n= (\frac{T_{1-\alpha} + T_{1-\beta}}{\frac{ES}{\sigma}})^2\]</span></p>
<p>Plugging in our known values (and looking up the T statistics using R using the qt() function), we can get the sample size required to obtain a power of 0.80.</p>
<pre class="r"><code>qt(0.95, df = 9)</code></pre>
<pre><code>[1] 1.833113</code></pre>
<pre class="r"><code>qt(0.8, df = 9)</code></pre>
<pre><code>[1] 0.8834039</code></pre>
<p><span class="math display">\[n = (\frac{2.716517}{\frac{1.5}{3}})^2 = 29.51786 \approx 30\]</span></p>
<p>We need a sample size of at least 30 birds to obtain our desired power.</p>
<div id="notes" class="section level3">
<h3>Notes</h3>
<ul>
<li><p>One obvious question to ask yourself is how we should base our estimates for effect size and sample variance before we have even performed an experiment. Depending on the situation, a few options are available to better come up with useful estimates for these quantities. For one, a pilot study could be performed. Or, if your lab has experience performing experiments similar to the one you are about to undertake, looking at the effect sizes or sample variances measured in previous studies is a very helpful strategy. Alternatively, looking into the literature to see examples of previous studies may be useful.</p></li>
<li><p>Note also that there are a variety of software packages available to automatically perform power calculations for you if you are able to supply estimates for the parameters of your sample/test. For example, <a href="http://rcompanion.org/rcompanion/index.html" class="uri">http://rcompanion.org/rcompanion/index.html</a> provides sample R programs to perform power analyses for a variety of different types of tests. Alternatively, G*Power <a href="http://www.gpower.hhu.de/" class="uri">http://www.gpower.hhu.de/</a> is a popular piece of software that can be used.</p></li>
<li><p>In practice, it may be useful to calculate a range of values for the sample size in an <em>a priori</em> analysis in order to account for the uncertainty in the estimation of each of the parameters in your analysis. The software above offer convenient ways to do this.</p></li>
</ul>
</div>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<ul>
<li><p>Power is defined as the probability of rejecting the null hypothesis given that the null hypothesis is false (the probability of correctly rejecting the null hypothesis).</p></li>
<li><p>Power analysis is extremely important in study design and post study analysis.</p></li>
<li><p>Power analysis depends on good estimates or prior assumptions for certain parameters of a test and is therefore very unique to the particular experiment being performed. Power analyses are entirely hypothetical as they rely on estimates of these parameters.</p></li>
</ul>
<br>
<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-sessioninfo" data-toggle="collapse" data-target="#workflowr-sessioninfo" style="display: block;">
<span class="glyphicon glyphicon-wrench" aria-hidden="true"></span> Session information
</button>
</p>
<div id="workflowr-sessioninfo" class="collapse">
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 3.5.1 (2018-07-02)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS  10.14.5

Matrix products: default
BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] gridExtra_2.3   cowplot_0.9.3   forcats_0.3.0   stringr_1.3.1  
 [5] dplyr_0.7.6     purrr_0.2.5     readr_1.1.1     tidyr_0.8.1    
 [9] tibble_1.4.2    ggplot2_3.0.0   tidyverse_1.2.1

loaded via a namespace (and not attached):
 [1] tidyselect_0.2.4 haven_1.1.2      lattice_0.20-35  colorspace_1.3-2
 [5] htmltools_0.3.6  yaml_2.2.0       rlang_0.2.2      pillar_1.3.0    
 [9] glue_1.3.0       withr_2.1.2      modelr_0.1.2     readxl_1.1.0    
[13] bindrcpp_0.2.2   bindr_0.1.1      plyr_1.8.4       munsell_0.5.0   
[17] gtable_0.2.0     workflowr_1.3.0  cellranger_1.1.0 rvest_0.3.2     
[21] evaluate_0.11    labeling_0.3     knitr_1.20       broom_0.5.0     
[25] Rcpp_0.12.18     scales_1.0.0     backports_1.1.2  jsonlite_1.5    
[29] fs_1.2.7         hms_0.4.2        digest_0.6.16    stringi_1.2.4   
[33] grid_3.5.1       rprojroot_1.3-2  cli_1.0.0        tools_3.5.1     
[37] magrittr_1.5     lazyeval_0.2.1   crayon_1.3.4     whisker_0.3-2   
[41] pkgconfig_2.0.2  xml2_1.2.0       lubridate_1.7.4  assertthat_0.2.1
[45] rmarkdown_1.10   httr_1.3.1       rstudioapi_0.7   R6_2.2.2        
[49] nlme_3.1-137     git2r_0.23.0     compiler_3.5.1  </code></pre>
</div>
</div>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>

<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
http://docs.mathjax.org/en/latest/configuration.html.  This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
