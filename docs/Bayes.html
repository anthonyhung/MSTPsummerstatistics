<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Anthony Hung" />

<meta name="date" content="2019-05-06" />

<title>Bayesian Inference</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">ISTP Summer JC: Statistical Theory & Methods</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/anthonyhung/MSTPsummerstatistics">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->
<!-- Add a small amount of space between sections. -->
  <style type="text/css">
    div.section {
      padding-top: 12px;
    }
  </style>

<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Bayesian Inference</h1>
<h4 class="author">Anthony Hung</h4>
<h4 class="date">2019-05-06</h4>

</div>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#bayes-theorem">Bayes theorem</a><ul>
<li><a href="#derivation-of-bayes-theorem">Derivation of Bayes theorem</a></li>
<li><a href="#applying-bayes-theorem-example-of-screening-test">Applying Bayes theorem: Example of screening test</a></li>
<li><a href="#bayes-theorem-using-a-prior-probability-distribution">Bayes theorem using a prior probability distribution</a></li>
</ul></li>
<li><a href="#likelihoods-and-likelihood-ratios">Likelihoods and Likelihood ratios</a><ul>
<li><a href="#likelihood-vs-probability">Likelihood vs probability</a></li>
</ul></li>
<li><a href="#likelihood-ratios">Likelihood ratios</a><ul>
<li><a href="#problem">Problem</a></li>
<li><a href="#solution-likelihood-ratio">Solution: Likelihood ratio</a></li>
</ul></li>
<li><a href="#bayesian-inference">Bayesian inference</a><ul>
<li><a href="#example-of-bayesian-inference-in-selecting-between-two-models">Example of Bayesian inference in selecting between two models:</a></li>
</ul></li>
<li><a href="#exercise-bayes-rule-using-probability-distributions">Exercise: Bayes Rule using probability distributions</a></li>
<li><a href="#summary">Summary</a></li>
</ul>
</div>

<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span> workflowr <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks"> Checks <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> </a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2019-07-02
</p>
<p>
<strong>Checks:</strong> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> 5 <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> 1
</p>
<p>
<strong>Knit directory:</strong> <code>MSTPsummerstatistics/</code> <span class="glyphicon glyphicon-question-sign" aria-hidden="true" title="This is the local directory in which the code in this file was executed."> </span>
</p>
<p>
This reproducible <a href="http://rmarkdown.rstudio.com">R Markdown</a> analysis was created with <a
  href="https://github.com/jdblischak/workflowr">workflowr</a> (version 1.3.0). The <em>Checks</em> tab describes the reproducibility checks that were applied when the results were created. The <em>Past versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguncommittedchanges"> <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> <strong>R Markdown file:</strong> uncommitted changes </a>
</p>
</div>
<div id="strongRMarkdownfilestronguncommittedchanges" class="panel-collapse collapse">
<div class="panel-body">
<p>The R Markdown file has unstaged changes. To know which version of the R Markdown file created these results, you’ll want to first commit it to the Git repo. If you’re still working on the analysis, you can ignore this warning. When you’re finished, you can run <code>wflow_publish</code> to commit the R Markdown file and build the HTML.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongEnvironmentstrongempty"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Environment:</strong> empty </a>
</p>
</div>
<div id="strongEnvironmentstrongempty" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSeedstrongcodesetseed20180927code"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Seed:</strong> <code>set.seed(20180927)</code> </a>
</p>
</div>
<div id="strongSeedstrongcodesetseed20180927code" class="panel-collapse collapse">
<div class="panel-body">
<p>The command <code>set.seed(20180927)</code> was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSessioninformationstrongrecorded"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Session information:</strong> recorded </a>
</p>
</div>
<div id="strongSessioninformationstrongrecorded" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Recording the operating system, R version, and package versions is critical for reproducibility.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongCachestrongnone"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Cache:</strong> none </a>
</p>
</div>
<div id="strongCachestrongnone" class="panel-collapse collapse">
<div class="panel-body">
<p>Nice! There were no cached chunks for this analysis, so you can be confident that you successfully produced the results during this run.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsgithubcomanthonyhungMSTPsummerstatisticstree397882ba5bb902738ccf3b8e3dd320b85eeb4bectargetblank397882ba"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Repository version:</strong> <a href="https://github.com/anthonyhung/MSTPsummerstatistics/tree/397882ba5bb902738ccf3b8e3dd320b85eeb4bec" target="_blank">397882b</a> </a>
</p>
</div>
<div id="strongRepositoryversionstrongahrefhttpsgithubcomanthonyhungMSTPsummerstatisticstree397882ba5bb902738ccf3b8e3dd320b85eeb4bectargetblank397882ba" class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility. The version displayed above was the version of the Git repository at the time these results were generated. <br><br> Note that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use <code>wflow_publish</code> or <code>wflow_git_commit</code>). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    .DS_Store
    Ignored:    .RData
    Ignored:    .Rhistory
    Ignored:    .Rproj.user/
    Ignored:    analysis/.RData
    Ignored:    analysis/.Rhistory

Unstaged changes:
    Modified:   analysis/Bayes.Rmd
    Modified:   analysis/CLT.Rmd
    Modified:   analysis/introR.Rmd
    Modified:   analysis/markov.Rmd

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the R Markdown and HTML files. If you’ve configured a remote Git repository (see <code>?wflow_git_remote</code>), click on the hyperlinks in the table below to view them.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/397882ba5bb902738ccf3b8e3dd320b85eeb4bec/docs/Bayes.html" target="_blank">397882b</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-30
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/2debade6b3d2a6dc524f171e5aa30bd2a2156fcb/analysis/Bayes.Rmd" target="_blank">2debade</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-30
</td>
<td>
commit before republish
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/2debade6b3d2a6dc524f171e5aa30bd2a2156fcb/docs/Bayes.html" target="_blank">2debade</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-30
</td>
<td>
commit before republish
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/6d3e1c816c22dcdf898173d8e2858c3a3d0238ef/docs/Bayes.html" target="_blank">6d3e1c8</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-28
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/4fcd1d02795634da2ad4fe5a312d4f3cea747633/analysis/Bayes.Rmd" target="_blank">4fcd1d0</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-28
</td>
<td>
commit before republish
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/4fcd1d02795634da2ad4fe5a312d4f3cea747633/docs/Bayes.html" target="_blank">4fcd1d0</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-28
</td>
<td>
commit before republish
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/c117ef10cb90e0b3b89106a56e0e03762571345a/docs/Bayes.html" target="_blank">c117ef1</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-27
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/c3c7b6e2c616e5e9b36e302b69daebdd1718cae7/analysis/Bayes.Rmd" target="_blank">c3c7b6e</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-27
</td>
<td>
Bayesian
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/c3c7b6e2c616e5e9b36e302b69daebdd1718cae7/docs/Bayes.html" target="_blank">c3c7b6e</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-27
</td>
<td>
Bayesian
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/b291d24b99263105227f9a3edb002e98252785de/docs/Bayes.html" target="_blank">b291d24</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-24
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/a321d7b2e20826590e55b0957b026534e845ecaf/analysis/Bayes.Rmd" target="_blank">a321d7b</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-24
</td>
<td>
commit before republish
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/a321d7b2e20826590e55b0957b026534e845ecaf/docs/Bayes.html" target="_blank">a321d7b</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-24
</td>
<td>
commit before republish
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/c4bdfdc7ed061819bbb246828e4af30054e26b39/docs/Bayes.html" target="_blank">c4bdfdc</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-22
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/dd1e4114724e90bc7711b0f563b508f043146cd2/analysis/Bayes.Rmd" target="_blank">dd1e411</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-22
</td>
<td>
before republishing syllabus
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/096760a626cc23032b925ccc238d64c8c991c8c7/docs/Bayes.html" target="_blank">096760a</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-18
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/da98ae8f6248396dfbe60499fbe402a628903369/docs/Bayes.html" target="_blank">da98ae8</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-17
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/239723ec3a2b2292484af14f0a209ba997b6cc7b/analysis/Bayes.Rmd" target="_blank">239723e</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-08
</td>
<td>
Update learning objectives
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/8860e5737b62132c7bac7959da994ae00186a388/analysis/Bayes.Rmd" target="_blank">8860e57</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-06
</td>
<td>
Finalized schedule for classes
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/anthonyhung/MSTPsummerstatistics/8860e5737b62132c7bac7959da994ae00186a388/docs/Bayes.html" target="_blank">8860e57</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-06
</td>
<td>
Finalized schedule for classes
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>Bayesian inference is a fancy term for educated guessing. In essesence, in Bayesian inference we take prior information that we know about the world and use it to evaluate data that we are presented with to draw educated conclusions about the world.</p>
<p>Our objectives today include learning about Bayes theorem and its connections to conditional probability, likelihood ratios for comparing specified models, and applying Bayes theorem to inference problems where we would like to compare between two models of reality.</p>
</div>
<div id="bayes-theorem" class="section level2">
<h2>Bayes theorem</h2>
<p>Before we delve into Bayesian inference, we must first start by covering Bayes Theorem. Bayes Theorem is a result that comes from probability and describes the relationship between conditional probabilities.</p>
<p>Let us define A and B as two separate types of events. P(A|B), or “the probability of A given B” denotes the conditional probability of A occurring if we know that B has occurred. Likewise, P(B|A) denotes “the probability of B given A”. Bayes theorem relates P(A|B) and P(B|A) in a deceptively simple equation.</p>
<div id="derivation-of-bayes-theorem" class="section level3">
<h3>Derivation of Bayes theorem</h3>
<p><a href="https://oracleaide.files.wordpress.com/2012/12/ovals_pink_and_blue1.png" class="uri">https://oracleaide.files.wordpress.com/2012/12/ovals_pink_and_blue1.png</a></p>
<p>From our definition of conditional probability, we know that P(A|B) can be defined as the probability that A occurs given that B has occured. This can be written mathematically as:</p>
<p><span class="math display">\[P(A|B) = \frac{P(A \cap B )}{P(B)}\]</span></p>
<p>Here, <span class="math inline">\(\cap\)</span> denotes the intersection between A and B (i.e. “A AND B occur together”). To calculate the probability of A conditional on B, we first need to find the probability that B has occured. Then, we need to figure out out of the situations where B has occured, how often does A also occur?</p>
<p>In a similar way, we can write P(B|A) mathematically:</p>
<p><span class="math display">\[P(B|A) = \frac{P(B \cap A )}{P(A)}\]</span></p>
<p>Since <span class="math inline">\(P(B \cap A )=P(A \cap B)\)</span> (does this make sense?), we can combine the two equations:</p>
<p><span class="math display">\[P(A|B)P(B) = P(B \cap A ) = P(B|A)P(A)\]</span></p>
<p>If we divide both sides by P(B):</p>
<p><span class="math display">\[P(A|B) = \frac{P(B|A )P(A)}{P(B)}\]</span></p>
<p>This is Bayes theorem! Notice that using this equation, we can connect the two conditional probabilities. Oftentimes, knowing this relationship is extremely useful because we will know P(B|A) but want to compute P(A|B). Let’s explore an example.</p>
</div>
<div id="applying-bayes-theorem-example-of-screening-test" class="section level3">
<h3>Applying Bayes theorem: Example of screening test</h3>
<p>Let us assume that a patient named John goes to a see a doctor to undergo a screening test for an infectious disease. The test that is performed has been previously researched, and it is known to have a 99% reliability when administered to patients like John. In other words, 99% of sick people test positive in the test and 99% of healthy people test negative. The doctor has prior knowledge that 1% of people in general will have the disease in question. If the patient tests positive, what are the chances that he is sick?</p>
<p><hide>50%</hide></p>
<p>Let’s solve this problem through applying Bayes theorem:</p>
<p><span class="math display">\[P(A|B) = \frac{P(B|A )P(A)}{P(B)}\]</span></p>
<p>First, we need to define what we mean by each of the terms as they relate to the question.</p>
<p>We can define P(A) as the probability that John is sick, and P(B) as the probability that John tests positive. If John is a normal every-day person, then we can use the Doctor’s knowledge that 1% of the population is sick as P(A).</p>
<p>P(A|B), the probability that John is sick given that he tests positive, is exactly what we want to calculate!</p>
<p>However, we are given only the other conditional probability, P(B|A) or the probability that a sick person tests positive (0.99).</p>
<p>Great. We have all the quantities we need to plug in to Bayes theorem to get our answer, except for P(B), the probability that a person tests positive. In order to calculate this, we can use the equation:</p>
<p><span class="math display">\[P(B) = P(B|A)P(A) + P(B| A^c)P(A^c)\]</span></p>
<p>Here, <span class="math inline">\(A^c\)</span> denotes A not happening, in this case “John is not sick.” In essence, A can either be true or not, so the two terms should capture all possible scenarios where B can occur. In this case, what is <span class="math inline">\(P(B|A^c)\)</span>?</p>
<p><span class="math display">\[P(A|B) = \frac{P(B|A )P(A)}{P(B|A)P(A) + P(B| A^c)P(A^c)}\]</span></p>
<p>Notice that the first term in the denominator is the same as the numerator. Let’s plug in our values and solve the problem.</p>
<p><span class="math display">\[P(A|B) = \frac{(0.99)(0.01)}{(0.99)(0.01) + (0.01)(0.99)} = \frac{1}{2}\]</span></p>
</div>
<div id="bayes-theorem-using-a-prior-probability-distribution" class="section level3">
<h3>Bayes theorem using a prior probability distribution</h3>
<p>Note that we usually will not have a clean, single point statistic summary of the prior probability of an event. For example, in the disease screening example above we are told that the population prevalence of the disease is known and measured to be 0.01. However, as with all statistics, there will be a confidence interval associated with that measurement. Oftentimes, when we use Bayesian statistics we want to be able to capture the uncertainty in our estimates of parameters that are used in our calculations to give a more accurate picture of the confidence we have in our inferences. In fact, this advantage is the major motivating factor behind Bayesian statistics in the first place. Using a probability distribution as our prior probability will lead to our posterior probability being a distribution as well, thereby capturing the uncertainty we have in our inference, or “guess,” at the model parameters in question. If we have time at the end of class we will explore an example of this in an exercise.</p>
</div>
</div>
<div id="likelihoods-and-likelihood-ratios" class="section level2">
<h2>Likelihoods and Likelihood ratios</h2>
<div id="likelihood-vs-probability" class="section level3">
<h3>Likelihood vs probability</h3>
<div id="probability" class="section level4">
<h4>Probability</h4>
<p>Recall from our previous class on probability distributions that the definition of probability can be visualized as the area under the curve of a probability distribution. For example, let’s say that we have a fair coin (P(heads) = 0.5) and we flip it 30 times:</p>
<pre class="r"><code>library(ggplot2)</code></pre>
<pre><code>Warning: package &#39;ggplot2&#39; was built under R version 3.5.2</code></pre>
<pre class="r"><code>library(cowplot)</code></pre>
<pre><code>Warning: package &#39;cowplot&#39; was built under R version 3.5.2</code></pre>
<pre><code>
Attaching package: &#39;cowplot&#39;</code></pre>
<pre><code>The following object is masked from &#39;package:ggplot2&#39;:

    ggsave</code></pre>
<pre class="r"><code>library(grid)

x1  &lt;- 5:25
df &lt;- data.frame(x = x1, y = dbinom(x1, 30, 0.5))

ggplot(df, aes(x = x, y = y)) + 
  geom_bar(stat = &quot;identity&quot;, col = &quot;red&quot;, fill = c(&quot;white&quot;)) +
  scale_y_continuous(expand = c(0.01, 0)) + xlab(&quot;number of heads&quot;) + ylab(&quot;Density&quot;)</code></pre>
<p><img src="figure/Bayes.Rmd/unnamed-chunk-1-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-1-1">
Past versions of unnamed-chunk-1-1.png
</button>
</p>
<div id="fig-unnamed-chunk-1-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/c3c7b6e2c616e5e9b36e302b69daebdd1718cae7/docs/figure/Bayes.Rmd/unnamed-chunk-1-1.png" target="_blank">c3c7b6e</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-27
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>If we would like to find the probability that we would get more than 20 heads in 30 flips, we could calculate the area represented by bars that are greater than 18 on the x axis:</p>
<pre class="r"><code>ggplot(df, aes(x = x, y = y)) + 
  geom_bar(stat = &quot;identity&quot;, col = &quot;red&quot;, fill = c(rep(&quot;white&quot;, 14), rep(&quot;red&quot;, 7))) +
  scale_y_continuous(expand = c(0.01, 0)) + xlab(&quot;number of heads&quot;) + ylab(&quot;Density&quot;)</code></pre>
<p><img src="figure/Bayes.Rmd/unnamed-chunk-2-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-2-1">
Past versions of unnamed-chunk-2-1.png
</button>
</p>
<div id="fig-unnamed-chunk-2-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/c3c7b6e2c616e5e9b36e302b69daebdd1718cae7/docs/figure/Bayes.Rmd/unnamed-chunk-2-1.png" target="_blank">c3c7b6e</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-27
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Similarly, we could calculate the probability that we get between 9 and 13 heads:</p>
<pre class="r"><code>ggplot(df, aes(x = x, y = y)) + 
  geom_bar(stat = &quot;identity&quot;, col = &quot;red&quot;, fill = c(rep(&quot;white&quot;, 5), rep(&quot;red&quot;, 4), rep(&quot;white&quot;, 12))) +
  scale_y_continuous(expand = c(0.01, 0)) + xlab(&quot;number of heads&quot;) + ylab(&quot;Density&quot;)</code></pre>
<p><img src="figure/Bayes.Rmd/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-3-1">
Past versions of unnamed-chunk-3-1.png
</button>
</p>
<div id="fig-unnamed-chunk-3-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/c3c7b6e2c616e5e9b36e302b69daebdd1718cae7/docs/figure/Bayes.Rmd/unnamed-chunk-3-1.png" target="_blank">c3c7b6e</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-27
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>In each case, notice that the shape of the distribution does not change. The only thing that changes is the area that we shade in. In mathematical terms, in the first case we are calculating:</p>
<p><span class="math display">\[P(num\_heads &gt; 20 | Binom(n=30, p=0.5))\]</span></p>
<p>and in the second:</p>
<p><span class="math display">\[P(9&lt; num\_heads &lt; 13 | Binom(n=30, p=0.5))\]</span></p>
<p>What is changing is the left side of the | . The shape of the distribution stays the same. When we discuss probabilities, we are talking about the areas under a <strong>fixed distribution (model)</strong>.</p>
</div>
<div id="likelihood" class="section level4">
<h4>Likelihood</h4>
<p>So what about likelihood? Before we look at it graphically, let’s define what we mean by the term. “The likelihood for a model is the probability of the data under the model.” Mathematically,</p>
<p><span class="math display">\[L(Model;Data) = P(Data|Model)\]</span></p>
<p>This may look the same as what we did before, but in this case our <strong>data are fixed</strong>, not the distribution. Instead of asking, “If I keep my distribution constant, what is the probability of observing something?” with likelihood we are asking “Given that I have collected some data, how well does a certain distribution fit the data?”</p>
<p>Let’s assume the same situation we did for probability with the coin. In this case, we do not know if the coin is actually fair (P(heads = 0.5), or if it is rigged (e.g. P(heads = 0.6). We flip the coin 30 times and observe 20 heads.</p>
<p>What is the likelihood for our <strong>fair model</strong> (<span class="math inline">\(Binom(n=30, p=0.5)\)</span>) given that we observe these data? In other words, how well does the model as paramterized fit our observations?</p>
<p><span class="math display">\[L(Model;Data) = P(num\_heads = 20|Binom(n=30, p=0.5))\]</span></p>
<p>Let’s look at this graphically.</p>
<pre class="r"><code>ggplot(df, aes(x = x, y = y)) + 
  geom_bar(stat = &quot;identity&quot;, col = &quot;red&quot;, fill = c(rep(&quot;white&quot;, 15), rep(&quot;red&quot;, 1), rep(&quot;white&quot;, 5))) +
  scale_y_continuous(expand = c(0.01, 0)) + xlab(&quot;number of heads&quot;) + ylab(&quot;Density&quot;)</code></pre>
<p><img src="figure/Bayes.Rmd/unnamed-chunk-4-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-4-1">
Past versions of unnamed-chunk-4-1.png
</button>
</p>
<div id="fig-unnamed-chunk-4-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/c3c7b6e2c616e5e9b36e302b69daebdd1718cae7/docs/figure/Bayes.Rmd/unnamed-chunk-4-1.png" target="_blank">c3c7b6e</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-27
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We can also compute the exact probability using the “dbinom” function in R.</p>
<pre class="r"><code>dbinom(x = 20, size = 30, prob = 0.5)</code></pre>
<pre><code>[1] 0.0279816</code></pre>
<p>Okay. How well does our data fit a <strong>rigged coin</strong> model, where the P(heads = 0.6)? What is the likelihood for the rigged coin model given our data?</p>
<p><span class="math display">\[L(Model;Data) = P(num\_heads = 25|Binom(n=30, p=0.6))\]</span></p>
<p>Let’s look at this graphically.</p>
<pre class="r"><code>x1  &lt;- 5:25
df_rigged &lt;- data.frame(x = x1, y = dbinom(x1, 30, 0.6))

ggplot(df_rigged, aes(x = x, y = y)) + 
  geom_bar(stat = &quot;identity&quot;, col = &quot;red&quot;, fill = c(rep(&quot;white&quot;, 15), rep(&quot;red&quot;, 1), rep(&quot;white&quot;, 5))) +
  scale_y_continuous(expand = c(0.01, 0)) + xlab(&quot;number of heads&quot;) + ylab(&quot;Density&quot;)</code></pre>
<p><img src="figure/Bayes.Rmd/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-6-1">
Past versions of unnamed-chunk-6-1.png
</button>
</p>
<div id="fig-unnamed-chunk-6-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/c3c7b6e2c616e5e9b36e302b69daebdd1718cae7/docs/figure/Bayes.Rmd/unnamed-chunk-6-1.png" target="_blank">c3c7b6e</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-27
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>We can also compute the exact probability using the “dbinom” function in R.</p>
<pre class="r"><code>dbinom(x = 20, size = 30, prob = 0.6)</code></pre>
<pre><code>[1] 0.1151854</code></pre>
<p>It looks like the likelihood for the rigged coin model is higher! But is it high enough for us to say that the coin is rigged? Under both models, the probability of obtaining 20 heads is not that high, and we may want to base our conclusions on more robust statistics than a simple “it’s higher, therefore it’s true” heuristic. One way we can compare likelihoods is through likelihood ratios.</p>
</div>
</div>
</div>
<div id="likelihood-ratios" class="section level2">
<h2>Likelihood ratios</h2>
<div id="problem" class="section level3">
<h3>Problem</h3>
<p>Let’s explore an example of two bags containing white and black balls. Both bags contain 100 balls, but Bag #1 contains 50 white and 50 black balls, while Bag #2 contains 75 white and 25 black balls.</p>
<p>You are given one of the two bags and draw 10 balls from the bag (with replacement, putting the chosen ball back into the bag after each time you draw a ball). From doing this, you get: {BWBWBWWWWW}. Which bag do you have?</p>
</div>
<div id="solution-likelihood-ratio" class="section level3">
<h3>Solution: Likelihood ratio</h3>
<p>Here, we are trying to decide between two models (bag #1 and bag #2), that could have generated our observed data ({BWBWBWWWWW}). Let’s calculate the likelihoods for each model (<span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span>). Question: What type of distribution should we use to model the result of a single draw from a bag? <hide>A bernoulli distribution would be appropriate here, since we are modeling the binary result of a single trial.</hide> Since the result of each draw is independent from the results of all other draws, we can take the total probability as a product of all the joint probabilities.</p>
<p><span class="math display">\[P(x|M_1) = \prod\limits_ip_{1}^{x_i} (1-p_1)^{1-x_i}\]</span></p>
<p>and</p>
<p><span class="math display">\[P(x|M_2) = \prod\limits_ip_{2}^{x_i} (1-p_2)^{1-x_i}\]</span></p>
<p>where x is a vector of 0s and 1s representing black and white balls respectively, and i denotes a single observation (single draw of a ball from the mystery bag). In other words, <span class="math inline">\(x_1 = 0\)</span> means that the first ball drawn was black. <span class="math inline">\(p_{1}, p_{2}\)</span> denote the probability of drawing a white ball from bag 1 and bag 2 respectively (<span class="math inline">\(p_{1} = 0.5, p_{2} = 0.75\)</span>).</p>
<p>Plugging in numbers,</p>
<p><span class="math display">\[P(x|M_1) = 0.5*0.5*0.5*0.5*0.5*0.5*0.5*0.5*0.5*0.5 = 0.0009765625
\]</span></p>
<p><span class="math display">\[P(x|M_2) = 0.25*0.75*0.25*0.75*0.25*0.75*0.75*0.75*0.75*0.75 = 0.002085686
\]</span></p>
<p>Notice that in addition to <span class="math inline">\(M_1\)</span>’s likelihood being larger, both likelihood values are extremely small. Why might that be the case? <hide>We are multiplying many probabilities that are smaller than 1, so our result will be very small.</hide> This is why likelihood values alone are meaningless, since their absolute magnitude will really depend on the amount of data you have collected. This is also why it is important when comparing two likelihoods, they must have been calculated from the same data! However, interpreting the <strong>relative</strong> magnitude of likelihoods is productive when comparing two or more different models. We can compare them using a likelihood ratio.</p>
<p>The likelihood ratio comparing <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span> is defined as:</p>
<p><span class="math display">\[LR(M_1, M_2) := \frac{L(M_1; Data)}{L(M_2; Data)}\]</span></p>
<p>Large values of <span class="math inline">\(LR(M_1, M_2)\)</span> indicate support for M1. Small values of <span class="math inline">\(LR(M_1, M_2)\)</span> indicate support for model <span class="math inline">\(M_2\)</span>. For example, a LR = 3 means that the data support <span class="math inline">\(M_1\)</span> by a factor of 3.</p>
<p>The inverse of <span class="math inline">\(LR(M_1, M_2)\)</span> is <span class="math inline">\(LR(M_2, M_1)\)</span>.</p>
<p><span class="math display">\[LR(M_1, M_2) = \frac{1}{LR(M_2, M_1)} = \frac{L(M_1; Data)}{L(M_2; Data)}\]</span></p>
<p>In our example, the <span class="math inline">\(LR(M_1, M_2) = \frac{L(M_1; Data)}{L(M_2; Data)} = \frac{0.0009765625}{0.002085686}= 0.4682212\)</span> <span class="math inline">\(LR(M_2, M_1) = \frac{1}{LR(M_1, M_2)} = 2.1356\)</span>. In other words, the data favor bag #2 by a factor of 2.14. Notice that in contrast to just comparing the magnitude of the likelihoods to see which one is larger, the LR allows you to have a tangible interpretation of how much larger one likelihood is than another and what this means for the amount of support for each model. Note that in the above example, our two models are fully specified (we know exactly how many balls of each type are in each bag). In cases where one or more of the models being compared are not fully specified and instead parameters must be estimated from the data, simply comparing two likelihood ratios is not sufficient and instead a likelihood ratio test (LRT: <a href="https://en.wikipedia.org/wiki/Likelihood-ratio_test" class="uri">https://en.wikipedia.org/wiki/Likelihood-ratio_test</a>) must be employed. LRTs are based on an asymptotic approximation of a test statistic involving the LR that approches a <span class="math inline">\(\chi^2\)</span> distribution.</p>
</div>
</div>
<div id="bayesian-inference" class="section level2">
<h2>Bayesian inference</h2>
<p>Bayes theorem gives us an alternative way to compare two models that incorporates not only the likelihood ratio, but also the prior information in support of a certain model.</p>
<p>Let’s return to Bayes theorem. Recall the statement:</p>
<p><span class="math display">\[P(A|B) = \frac{P(B|A )P(A)}{P(B)}\]</span></p>
<p>If we define B to be our observed data, then Bayes theorem becomes:</p>
<p><span class="math display">\[P(A|Data) = \frac{P(Data|A )P(A)}{P(Data)}\]</span></p>
<p>Notice the <span class="math inline">\(P(Data|A)\)</span> term is something we’ve talked about earlier, the likelihood. “The likelihood for a model is the probability of the data under the model.” With that in mind, we can now attach names to each of the terms in the equation.</p>
<ul>
<li><p>P(A|Data) is known as the posterior probability, or the probability of a model A given some observations.</p></li>
<li><p>P(Data|A) is known as the likelihood.</p></li>
<li><p>P(A) is known as the prior probability, or the probability of A before we have the observations.</p></li>
<li><p>P(Data) is the prior probability that the data themselves are true.</p></li>
</ul>
<p>Let us say we have two possible models <span class="math inline">\(A_0\)</span> and <span class="math inline">\(A_1\)</span> that could have generated our data that we would like to pick between.</p>
<p><span class="math display">\[P(A_0|Data) = \frac{P(Data|A_0 )P(A_0)}{P(Data)}\]</span></p>
<p>and</p>
<p><span class="math display">\[P(A_1|Data) = \frac{P(Data|A_1 )P(A_1)}{P(Data)}\]</span></p>
<p>If we take the ratio of the two posterior probabilities, we get:</p>
<p><span class="math display">\[\frac{P(A_0|Data)}{P(A_1|Data)} = \frac{\frac{P(Data|A_0 )P(A_0)}{P(Data)}}{\frac{P(Data|A_1 )P(A_1)}{P(Data)}} = \frac{P(Data|A_0 )P(A_0)}{P(Data|A_1 )P(A_1)} = \frac{P(Data|A_0 )}{P(Data|A_1 )}\cdot \frac{P(A_0)}{P(A_1)}\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[\frac{P(A_0|Data)}{P(A_1|Data)} = \frac{P(A_0)}{P(A_1)} \cdot \frac{P(Data|A_0 )}{P(Data|A_1 )}\]</span></p>
<p>Written out in words and using the term “Odds” to represent the ratio of the probabilities of the two models, we get:</p>
<center>
Posterior Odds = Prior Odds * Likelihood Ratio
</center>
<p>Just like before, the <strong>prior</strong> odds refers to the ratio of the probabilties of the two models before considering the observed data, and the <strong>posterior</strong> odd refers to the ratio of the probabilities after considering the observed data.</p>
<p>Now we can appreciate how viewing this model selection problem using Bayesian inference holds some advantages over our previous strategy of simply using the Likelihood ratio. Suppose that one of your models is much more likely to be true than the other based on prior evidence or belief. For example, in the 2 bag example we explored earlier, imagine that you had been given a hint earlier that the person in charge of the bag game almost always picks bag #1 to give to the participant and almost never picks bag #2. If you were to simply calculate the likelihood ratio to compare the two models of reality, you would not be able to factor in this extra information. The Prior odds term in the Bayesian case allows you to take into account extra information such as this to great effect. Let’s explore an example:</p>
<div id="example-of-bayesian-inference-in-selecting-between-two-models" class="section level3">
<h3>Example of Bayesian inference in selecting between two models:</h3>
<p>Let’s return to the example earlier of the two bags. Imagine that instead of there being two different bags, there were 10 different bags, 9 of bag type #1 (corresponding to containing 50 white and 50 black balls), and 1 of bag type #2 (corresponding to containing 75 white and 25 black balls). You draw 10 balls from a randomly chosen bag and they are of the colors {BWBWBWWWWW}. Let’s compute the posterior odds of the bag being of type #1 vs the bag being of type #2.</p>
<p><span class="math display">\[\frac{P(type_1|BWBWBWWWWW)}{P(type_2|BWBWBWWWWW)} = \frac{P(type_1)}{P(type_2)} \cdot \frac{P(BWBWBWWWWW|type_1 )}{P(BWBWBWWWWW|type_2 )}\]</span></p>
<p>We previously already calculated the second term (remember that it is just the LR). We know that there are 9 bags of type #1 and 1 bags of type #2. Thereore, the prior odds will be 9/1.</p>
<p><span class="math display">\[\frac{P(type_1|BWBWBWWWWW)}{P(type_2|BWBWBWWWWW)} = \frac{9}{1}\cdot 0.4682212 = 4.213991\]</span></p>
<p>Notice that our conclusion has flipped! The data, when considered in light of the prior information about the probability of a certain bag type being chosen, result in a conclusion that we would favor calling the bag in front of us a type #1 bag at an odds of 4.21 to 1.</p>
<p>Question: What happens if instead of 9 bags being of type #1 and 1 being of type #2, there were 5 bags of each type?</p>
<p><hide>Because the prior odds will be 1 (each bag type has the same prior probability), the prior odds term cancels out and we are left with just the likelihood ratio (exactly what we had already calculated ealier in the “Likelihood ratio” section. We would then favor calling the bag in front of us a type #2 bag.</hide></p>
</div>
</div>
<div id="exercise-bayes-rule-using-probability-distributions" class="section level2">
<h2>Exercise: Bayes Rule using probability distributions</h2>
<p>Let’s return to Bayes Rule and instead of plugging in a single point estimate for our prior probability, enter in a prior probability <em>distribution</em>.</p>
<p>In using Bayesian inference, there are four key steps to follow:</p>
<ol style="list-style-type: decimal">
<li>Identify the type of data you are working with (continuous, discrete, binary, etc)</li>
<li>Construct a model for your likelihood</li>
<li>Specify a prior distribution for the parameter of interest</li>
<li>Apply Bayes Rule (Posterior <span class="math inline">\(\propto\)</span> Prior * Likelihood)</li>
</ol>
<p>Let’s say we have a coin in front of us and we are not sure what the true probability of getting a heads using this coin is. We would like to infer what the probability is using Bayesian inference. Question: How would we estimate the probability of getting heads using frequentist statistics? <hide>Flip it a bunch of times and count the number of heads. Divide the number by the total number of flips and get a proportion.</hide></p>
<p>Let’s follow the steps for Bayesian inference.</p>
<ol style="list-style-type: decimal">
<li><p>We are going to flip this coin to collect data on it, so our collected data will be in the form of heads and tails (binary data if we represent heads and tails as 1s and 0s respectively). If we flip the coin mulitple times, then our data will look like count data (i.e. the number of heads we get in X flips).</p></li>
<li><p>What would be a good distribution to model the probability of obtaining a certain number of heads from an X number of coin flips? <hide>Binomial distribution</hide> We will use this distribution to model our likelihood. The Binomial likelihood function looks like this:</p></li>
</ol>
<p><span class="math display">\[L(p | n, k) = f(k;\theta) =\binom{n}{k}\theta^k (1-\theta)^{n-k}\]</span></p>
<p>Where k represents number of heads, n represents the total number of coin flips, and <span class="math inline">\(\theta\)</span> represents the probability of getting a heads in a single trial. Notice that the Binomial ikelihood function is just the probability mass function of the binomial distribution.</p>
<ol start="3" style="list-style-type: decimal">
<li>What is our prior distribution for the probability of a heads from a single flip of this mystery coin? Since we do not have any idea about what the prior probability of heads is, we can use a uniform prior (a “flat” or “uniformative” prior). A flat prior in this case means that we do not have any reason to believe that one value of P(heads) is more likely to be true than any other. To be precise, we will use a beta prior distribution (<a href="https://en.wikipedia.org/wiki/Beta_distribution" class="uri">https://en.wikipedia.org/wiki/Beta_distribution</a>) with parameters (1,1), which is the same thing as a uniform distribution when evaluated between 0 and 1 (see the graphs below). Since probabilties must be between 0 and 1, this is an appropriate substitution to make. It will make sense why we are using this funny distribution later. (Note that we don’t need to use a named distribution as our prior distribution. We could distribute the weights we want to apply to each possible value of P(heads) however we like and still use Bayesian inference.)</li>
</ol>
<pre class="r"><code>ggplot(data = data.frame(x = c(0, 1)), aes(x)) + 
  stat_function(fun = dunif, args = list(min=0, max=1 ), color = &quot;red&quot;, linetype = &quot;solid&quot;) + ggtitle(&quot;Uniform distribution Probabilty density function&quot;)</code></pre>
<p><img src="figure/Bayes.Rmd/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-8-1">
Past versions of unnamed-chunk-8-1.png
</button>
</p>
<div id="fig-unnamed-chunk-8-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/2debade6b3d2a6dc524f171e5aa30bd2a2156fcb/docs/figure/Bayes.Rmd/unnamed-chunk-8-1.png" target="_blank">2debade</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-30
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>ggplot(data = data.frame(x = c(0, 1)), aes(x)) + 
  stat_function(fun = dbeta, args = list(shape1 = 1, shape2 = 1), color = &quot;red&quot;, linetype = &quot;solid&quot;) + ggtitle(&quot;Beta distribution Probabilty density function with parameters (1,1)&quot;)</code></pre>
<p><img src="figure/Bayes.Rmd/unnamed-chunk-8-2.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-8-2">
Past versions of unnamed-chunk-8-2.png
</button>
</p>
<div id="fig-unnamed-chunk-8-2" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/2debade6b3d2a6dc524f171e5aa30bd2a2156fcb/docs/figure/Bayes.Rmd/unnamed-chunk-8-2.png" target="_blank">2debade</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-30
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>The PDF of the beta distribution looks like this (B represents the beta function, which is not important to consider for today’s discussion):</p>
<p><span class="math display">\[f(x;\alpha, \beta) = \frac{1}{B(\alpha, \beta)}x^{\alpha-1}(1-x)^{\beta-1}\]</span></p>
<p>Plugging in our parameter values for <span class="math inline">\(\alpha and \beta\)</span> and substituting <span class="math inline">\(\theta\)</span> for x, we get</p>
<p><span class="math display">\[f(\theta;1,1) \propto \theta^{1-1}(1-\theta)^{1-1} = 1\]</span></p>
<p>which is the same as the uniform distribution.</p>
<ol start="4" style="list-style-type: decimal">
<li>Now that we’ve specified our likelihood function and prior distribution, we can move ahead to generating data and plugging it into Bayes theorem!</li>
</ol>
<p>Imagine we flip the coin 100 times and get 40 heads. (n = 100, k = 40). Recall Bayes theorem:</p>
<p><span class="math display">\[P(A_0|Data) = \frac{P(Data|A_0 )P(A_0)}{P(Data)}\]</span></p>
<p>We can plug in our likelihood and prior distributions:</p>
<p><span class="math display">\[P(A_0|Data) = \frac{(\binom{100}{40}\theta^{40} (1-\theta)^{100-40})(\theta^{1-1}(1-\theta)^{1-1})}{P(Data)} \propto \theta^{40} (1-\theta)^{100-40})(\theta^{1-1}(1-\theta)^{1-1} = \theta^{40+1-1} (1-\theta)^{100-40+1-1}\]</span></p>
<p>Notice that we moved from an = to a <span class="math inline">\(\propto\)</span>. This is done because calculating the P(Data) is difficult, and so is including the math required to evaluate the Beta function and the 100 choose 40 function. However, in ignoring these extraneous normalizing constants (which exist in the equation in order to make our expression equal a probability that sums to 1), we gain a much simpler interpretation of our posterior probability. Recognize that the term <span class="math inline">\(\theta^{40+1-1} (1-\theta)^{100-40+1-1}\)</span> resembles the PDF for a beta distribution: <span class="math inline">\(f(x;\alpha, \beta) = \frac{1}{B(\alpha, \beta)}x^{1-\alpha}(1-x)^{\beta-1}\)</span>, minus the normalizing constant that contains the Beta function, if we set <span class="math inline">\(\alpha = 41\)</span> and <span class="math inline">\(\beta = 61\)</span>. Actually, due to a mathematical property that links the binomial and Beta distributions, we know for a fact that the posterior distribution is Beta distributed.</p>
<p>This is because the Beta distribution is a <strong>conjugate prior</strong> for the binomial distribution. This means that if the likelihood function is binomial and the prior distribution is a Beta distribution then the posterior distribution is also a Beta distribution. There are many pairs of distributions that have this relationship, and using these relationships allows us to easily work with posterior distributions that come out of applications of Bayes theorem. In cases where conjugate distributions are not used, more sophisticated methods such as Markov Chain Monte Carlo can be used to characterize the posterior distribution.</p>
<p>Since we now know that the posterior distribution is distributed as a Beta distribution with <span class="math inline">\(\alpha = 41\)</span> and <span class="math inline">\(\beta = 61\)</span>, we can fully determine the posterior distribution and fill in the normalizing constant.</p>
<p><span class="math display">\[P(A_0| Data) \sim Beta(\alpha = 41, \beta = 61) = \frac{1}{B(41, 61)}x^{41-1}(1-x)^{61-1}\]</span></p>
<p>Because we know the distribution, we can take advantage of known properties of the Beta distribution to easily determine the mean (<span class="math inline">\(\frac{\alpha}{\alpha+\beta}\)</span>), variance (<span class="math inline">\(\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}\)</span>), and other properties of the posterior distribution. We can also graph it:</p>
<pre class="r"><code>ggplot(data = data.frame(x = c(0, 1)), aes(x)) + 
  stat_function(fun = dbeta, args = list(shape1 = 41, shape2 = 61), color = &quot;red&quot;, linetype = &quot;solid&quot;) + ggtitle(&quot;Posterior distribution&quot;) + ylim(0, 8.5)</code></pre>
<p><img src="figure/Bayes.Rmd/unnamed-chunk-9-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-9-1">
Past versions of unnamed-chunk-9-1.png
</button>
</p>
<div id="fig-unnamed-chunk-9-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/2debade6b3d2a6dc524f171e5aa30bd2a2156fcb/docs/figure/Bayes.Rmd/unnamed-chunk-9-1.png" target="_blank">2debade</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-30
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Where is the mode of our distribution? Does this make sense given the data we were presented with?</p>
<p>What would happen to the shape of the posterior distribution if our data were less convincing (i.e. there was less data)? Let’s see if instead of 100 coin flips being fed into our inference, we only flipped the coin 20 times (and got 8 heads, or 40%):</p>
<pre class="r"><code>ggplot(data = data.frame(x = c(0, 1)), aes(x)) + 
  stat_function(fun = dbeta, args = list(shape1 = 9, shape2 = 13), color = &quot;red&quot;, linetype = &quot;solid&quot;) + ggtitle(&quot;Posterior distribution&quot;)+ ylim(0, 8.5)</code></pre>
<p><img src="figure/Bayes.Rmd/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-10-1">
Past versions of unnamed-chunk-10-1.png
</button>
</p>
<div id="fig-unnamed-chunk-10-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/2debade6b3d2a6dc524f171e5aa30bd2a2156fcb/docs/figure/Bayes.Rmd/unnamed-chunk-10-1.png" target="_blank">2debade</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-30
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>As you can see, if our data are less convincing, our confidence in the inference we have made about the probability of getting a heads from the coin is more spread out around the mode, reflecting our uncertainty in the estimate. In fact, if we have very little data (e.g. 5 coin flips), our distribution starts to look more and more like our prior distribution (flat).</p>
<pre class="r"><code>ggplot(data = data.frame(x = c(0, 1)), aes(x)) + 
  stat_function(fun = dbeta, args = list(shape1 = 3, shape2 = 4), color = &quot;red&quot;, linetype = &quot;solid&quot;) + ggtitle(&quot;Posterior distribution&quot;)+ ylim(0, 8.5)</code></pre>
<p><img src="figure/Bayes.Rmd/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-11-1">
Past versions of unnamed-chunk-11-1.png
</button>
</p>
<div id="fig-unnamed-chunk-11-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/2debade6b3d2a6dc524f171e5aa30bd2a2156fcb/docs/figure/Bayes.Rmd/unnamed-chunk-11-1.png" target="_blank">2debade</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-30
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>What happens if we have a huge amount of data (1000 flips, 400 heads)?</p>
<pre class="r"><code>ggplot(data = data.frame(x = c(0, 1)), aes(x)) + 
  stat_function(fun = dbeta, args = list(shape1 = 401, shape2 = 601), color = &quot;red&quot;, linetype = &quot;solid&quot;) + ggtitle(&quot;Posterior distribution&quot;)+ ylim(0, 8.5)</code></pre>
<p><img src="figure/Bayes.Rmd/unnamed-chunk-12-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-unnamed-chunk-12-1">
Past versions of unnamed-chunk-12-1.png
</button>
</p>
<div id="fig-unnamed-chunk-12-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/anthonyhung/MSTPsummerstatistics/blob/2debade6b3d2a6dc524f171e5aa30bd2a2156fcb/docs/figure/Bayes.Rmd/unnamed-chunk-12-1.png" target="_blank">2debade</a>
</td>
<td>
Anthony Hung
</td>
<td>
2019-05-30
</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>In essence, you can think about a Bayesian inference as a weighted average of our prior expectations about the world and the observations we have about the world. If we have a lot of confidence in our prior (a “strong” prior), it may take us a lot of evidence to persuade us to change our minds. Likewise, if we have very little evidence, we will place less weight on that evidence when making an inference. If we have a lot of evidence, we will be much more likely to rely on this evidence (the prior matters less and less). We can see an interactive illustration of this fact using this applet: <a href="http://stephens999.github.io/fiveMinuteStats/shiny_normal_example.html" class="uri">http://stephens999.github.io/fiveMinuteStats/shiny_normal_example.html</a> (the applet illustrates a situation where you have a normal prior and normal likelihood, and the normal distribution is a conjugate prior for itself!)</p>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>In using Bayesian inference, there are four key steps to follow:</p>
<ol style="list-style-type: decimal">
<li>Identify the type of data you are working with (continuous, discrete, binary, etc)</li>
<li>Construct a model for your likelihood</li>
<li>Specify a prior distribution for the parameter of interest</li>
<li>Apply Bayes Rule (Posterior <span class="math inline">\(\propto\)</span> Prior * Likelihood)</li>
</ol>
<p>The major motivations behind using Bayesian inference are to (1) allow you to take into account prior information you have about a particular model of how the world is when trying to make educated guesses about the world (2) to allow you to obtain a measure of how confident you are in the educated guesses you come up with using inference.</p>
<br>
<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-sessioninfo" data-toggle="collapse" data-target="#workflowr-sessioninfo" style="display: block;">
<span class="glyphicon glyphicon-wrench" aria-hidden="true"></span> Session information
</button>
</p>
<div id="workflowr-sessioninfo" class="collapse">
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 3.5.1 (2018-07-02)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS  10.14.5

Matrix products: default
BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] grid      stats     graphics  grDevices utils     datasets  methods  
[8] base     

other attached packages:
[1] cowplot_0.9.4 ggplot2_3.1.1

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.1       knitr_1.23       whisker_0.3-2    magrittr_1.5    
 [5] workflowr_1.3.0  tidyselect_0.2.5 munsell_0.5.0    colorspace_1.4-1
 [9] R6_2.4.0         rlang_0.3.4      dplyr_0.8.1      stringr_1.4.0   
[13] plyr_1.8.4       tools_3.5.1      gtable_0.3.0     xfun_0.7        
[17] withr_2.1.2      git2r_0.25.2     htmltools_0.3.6  assertthat_0.2.1
[21] yaml_2.2.0       lazyeval_0.2.2   rprojroot_1.3-2  digest_0.6.19   
[25] tibble_2.1.2     crayon_1.3.4     purrr_0.3.2      fs_1.3.1        
[29] glue_1.3.1       evaluate_0.14    rmarkdown_1.13   labeling_0.3    
[33] stringi_1.4.3    pillar_1.4.1     compiler_3.5.1   scales_1.0.0    
[37] backports_1.1.4  pkgconfig_2.0.2 </code></pre>
</div>
</div>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>

<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
http://docs.mathjax.org/en/latest/configuration.html.  This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
