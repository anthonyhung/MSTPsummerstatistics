---
title: "The Central Limit Theorem"
author: "Anthony Hung"
date: "2019-04-25"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

The central limit theorem (CLT) is a fundamental pillar to statistics and probability. Understanding it goes a long way to explain why we can make inferences about populations (biology) based on a limited number of samples (experiments) taken from those populations. Our objective today is to learn about what the CLT says and how this concept is used widely in science to apply statistical tests to sample data.

## Definitions

The CLT states that for sufficiently large samples taken from a population with any population distribution, the sampling distribution of the mean tends towards a normal distribution centered around the true mean of the population. The power of this statement is that it does not matter what shape the population distribution takes. If our sample size is large enough, the distribution of our samples looks like a bell curve with a mean that is approximately equal to the population mean. More formally, when a continuous variable is dependent on many independent small, random factors, measurements of that variable will approximate a Gaussian distribution.

To go more in depth, let's define what we mean by population and sampling distributions.

### Example: Studying the heights of a population of wild deer

Imagine we are rangers at a National Park and are charged with finding the average  height of all the deer (which number at 1,0000,000) in our park. One way to go about doing this would be go out one day and capture __all 1,000,000 deer in the park__ and measure their heights. If you were to do so, you may obtain a dataset that looks like this:

```{r deer population parameters}
deer_heights <- rbeta(1E6, 10, 1)*100

hist(deer_heights)
abline(v= mean(deer_heights), col="red")
mean(deer_heights)
```

What we can see is that the histogram, or the __population distribution__ of the 1,000,000 heights does not look like a bell curve, which means that the heights of the population of deer is not normally distributed. This is the population distibution, as it includes every single member of the population rather than representing a subset of the population. The mean of the population distribution of heights is 90.90923 (represented by the red line on the histogram). This the __population mean__.

Of course, you will almost never have the resources or time to be able to measure every single individual in a population you are interested in. For example, as a ranger you may go out and randomly capture a few representative samples, each containing 30 deer from your park, and measure their heights to compute the mean height within each sample. Each of the means of your samples is a __sample mean__, and plotting the many sample means you have computed will give you the __sampling distribution__.

Let's look first at individual sample means. Say we go out on 5 sampling expeditions, capturing 25 deer each time.

```{r single samples}
for(i in 1:5){
  print(mean(sample(deer_heights, 25, replace=F)))
}
```

The sample means from each of our 5 samples differ somewhat, due to the nature of random sampling. Some of the sample means are higher than the population mean, and some are lower. What happens if we take many more samples and plot their distribution?

Below, we take 500 samples of size 25 from our original deer population, compute the means of each of those samples, and plot the sampling distribution.

```{r sample deer}
sample_means <- c()

for(i in 1:500){
  sample_means <- c(sample_means, mean(sample(deer_heights, 25, replace=F)))
}
hist(sample_means,  freq = F, xlim = c(84, 98), main="Sampling distribution, n=25")
abline(v=mean(sample_means), col="red")
mean(sample_means)
```

We can appreciate that even though our __population distribution__ looked very left-skewed, our __sampling distribution__ looks much more bell-shaped and normal. The mean of the sampling distribution is 90.8481, which is very close to the true population mean. This, in essence, is what the CLT tells us. Even though our population distribution was not normally distributed, the shape of the sampling distribution tends to a normal distribution centered around the true population mean as the sample size increases. The CLT not only tell us what shape to expect for our sampling distribution, but also how spread out our sampling distribution is expected to be depending on our sample size. What happens if instead of sampling 25 deer at a time, we sample 100?

```{r sample deer 100}
sample_means <- c()

for(i in 1:500){
  sample_means <- c(sample_means, mean(sample(deer_heights, 100, replace=F)))
}
hist(sample_means,  xlim = c(84, 98), main="Sampling distribution, n=100", freq = F)
abline(v=mean(sample_means), col="red")
mean(sample_means)
```

By keeping both of our x-axis ranges constant, we can appreciate that the sampling distribution with sample sizes of 100 deer is much more tightly centered around the mean than the distribution with sample sizes of 25. 

#### Standard deviation of sampling distribution

Mathematically, the relationship between the sample size and the standard deviation of the sampling distribution as n tends towards large numbers is given by:

$$\sigma_{sampling} = \frac{\sigma}{\sqrt{n}}$$

The relationship between the sample distribution standard deviation and the sample size demonstrates why statistical inferences improve with larger sample sizes. As your n increases, you have more certainty in your estimates, since your sample statistics are more likely to be close to the true population parameters.

As a gut-check and to test this relationship, let us revisit our most recent plot of the sampling distribution for n=100. Here, I have overlaid a normal distribution with mean $\mu = \mu_{population}$ and standard deviation $\sigma = \frac{\sigma_{population}}{\sqrt{100}}$ in blue.

```{r}
hist(sample_means,  xlim = c(84, 98), main="Sampling distribution, n=100", freq = F)
curve(dnorm(x, mean = mean(sample_means), sd = sd(deer_heights)/sqrt(100)), add=T, col="blue")
abline(v=mean(sample_means), col="red")
```

## CLT and why the normal distribution is so prevalent

The CLT not only is useful for science in allowing us to apply many fundamental statistical tests to sampling distributions, but it also explains why we see the normal distribution so often in nature. For example, you may know that the height of people in a population is distributed normally. The reason behind this fascinating result is that the height of a person is contributed to by a large number of random variables, including nutrition, genetics, other aspects of environment, etc. Even just looking at the genetics piece of the equation, height is known as a highly polygenic trait, which means many many different loci across the genome contribute genetically to a person's height, and each locus can be thought of as a random variable. In the end, you can think of a person's height as the sum of the value of a large number of random variables. The CLT tells us that the sum of the distribution of a large number of random variables looks like a normal distribution, which explains this result.

## CLT $\ne$ Law of Large Numbers

The CLT is often confused with the definition for the Law of Large numbers, which states that as the size of an individual sample is increased, the more accurate of an estimate the sample statistic will be of a population parameter Taken to the near limit, you can imagine that if you were able to sample 999,999 deer and measure their heights, your sample estimate for the mean height of the entire deer population would most likely be very good!

In contrast, the CLT does not say anything about individual samples and their estimates of population parameters. Instead, it describes the shape of the sampling distribution, which encompasses what one would expect to see through taking many samples from the population.


## Key points about the CLT

* The CLT describes the characteristics of a __sampling distribution__, not individual samples themselves.

* The CLT allows us to apply many of the statistical tests that are commonly used to a wide variety of situations. For example, in nature there is no such thing as a perfectly normally distributed population. However, the CLT tells us that if we have a large enough sample size, we can always expect to see a normally distributed sampling distribution, which means we can apply statistical tests that assume normality on sampling distributions.

* The statement of the CLT allows us to use information we calculate based on a random representative sample to infer parameters of the population. This is of great usefulness to us a scientists, as it enables *inference*. Imagine if the only way to obtain accurate measurements of a population of cells, mice, or patients was to measure every single last one!

* The precision of the estimates we obtain from our samples depends very much on the sample size, or number of individuals contained within each sample. 

* The shape of the population distribution determines how large of a sample size is required for the CLT to hold. If a population is very far from normally distributed, you many need much large sample sizes for the CLT to hold.

* The fact that even with large sample size, our estimates of the population mean have some amount of variance around the true population mean gives us an idea of the importance of replicating experiments. Replication allows us to get some sense of how large the standard deviation of our sampling distribution is, and averaging across replicates increases the accuracy of our estimates.
