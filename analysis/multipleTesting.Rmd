---
title: "Multiple Testing Correction"
author: "Anthony Hung"
date: "2019-04-24"
output: 
  workflowr::wflow_html:
    code_folding: show
editor_options:
  chunk_output_type: console
  header-includes:
     - \usepackage{tabularx}
     - \usepackage{tcolorbox}
---

<style>
hide {
  background-color: #d6d6d6;
  color: #d6d6d6;
}
hide:hover {
  background-color: white;
  color: black;
}
</style>

## Introduction

Multiple testing describes situations where many hypotheses are simultaneously investigated from a given dataset. Correct treatment of statistics when working with multiple hypotheses is paramount, as mistakes can easily lead to false interpretations of results and many false positives. Our objectives today are to review the framework behind hypothesis testing in single hypotheses, why this framework falls apart in multiple testing, and different methods that have been proposed to correct for multiple testing.

## Hypothesis testing

The basic idea in hypothesis testing is to use data or observations to choose between two possible realities: a null hypothesis or an alternative hypothesis. As a reminder, the null hypothesis usually is the 'status quo' or less interesting view of the world. The alternative hypothesis is the opposite of the null hypothesis.

## The "issue" of multiple testing

As many scientific fields enter an age of "Big Data," where the ability to collect and work with data from a large number of measurements gives rise to the ability to test many hypotheses at the same time. However, as scientistists tests many more hypotheses, the standard view of hypothesis testing falls apart.

To illustrate this, consider the xkcd comic (https://xkcd.com/882/). Obviously, something is not right with the conclusions of the study, since we all have an intuition that green jelly beans do not have any true association with skin conditions. To better understand why mutliple testing can easily lead to false positive associations unless adequately treated, let us walk through the calculations for the probability of making a Type 1 error given the number of tests you are performing.

### Case 1: Performing 1 test

Let us say we are performing the study in the comic and testing for a link between purple jelly beans and acne at a significance level $\alpha = 0.05$. What is the probability that we make a type 1 error?

<hide> As $\alpha$ is equal to our type 1 error rate (the probability of rejecting the null hypothesis given the null hypothesis is true), we know that the probability is equal to 0.05.</hide>

### Case 2: Performing 20 tests

Now, let us test for an association between 20 different colors of jelly beans and acne at a significance level of $\alpha = 0.05$ for each individual test. What is the probability that we make at least one type 1 error now?

<hide> 
Here, we are interested in finding the P(making a type 1 error), which is the same as 1 - P(NOT making a type 1 error). The P(NOT making a type 1 error) for each of the individual tests is equal to $1- \alpha = 0.95$. If we assume that each of the separate tests is independent, then our probability of making at least one type 1 error amongst our 20 tests is $1-(1-0.05)^{20} = 0.6415$.
</hide>

This makes sense if we go back to the definition of a p-value. The p-value measures the probability of seeing an observation as extreme or more extreme than your data given that the data were drawn from a null distribution. If we have a p-value of 0.05, that is equivalent to saying that 5% of the time, in a universe where the null hypothesis is true, we would see data as extreme or more extreme that our data. (See drawing on chalkboard). If we perform 100 tests in a universe where the null hypothesis is true, then we expect 5 of those tests to have p-values less than 0.05! The number of expected false positives increases with the number of independent tests you perform. Clearly, we need to take into account the number of tests when setting our significance threshold. 

In the above two examples, what we calculated was the probability of making __at least one type 1 error__ in all the independent tests we were performing. This is otherwise known as the __Family-Wise Error Rate (FWER)__.

## Looking at multiple testing and FWER graphically

This graph plots the # of independent tests performed on the x axis, and the probability of seeing at least one type 1 error (the FWER) on the y axis, given an $\alpha = 0.05$.

```{r graph_FWER}
library(cowplot)
library(ggplot2)
ggplot(data.frame(x=c(0,60)), aes(x)) +
  stat_function(fun=function(x)1-(1-0.05)^x, geom="line", aes(colour="square")) + xlab("# of independent tests") + ylab("FWER") + theme(legend.position = "none")
```

As you can appreciate, the FWER increases quickly with every increase in the number of tests you are performing!

## Controlling type 1 error for situations with multiple tests:

We have two sets of options to control for our type 1 error rate when performing multiple tests:

1. Control the FWER to be under a certain value by setting an $\alpha_{corrected}$ for each individual test.

2. Control the __false discovery rate__ (FDR). In short, FDR methods control the proportion of false positive tests to all positive tests. 

### Bonferroni Correction

The Bonferroni correction is one of the most common and the simplest method for correcting for multiple testing, and it can be applied to cases where you are performing multiple dependent or independent tests. The theoretical basis behind this correction is to attempt to adjust our FWER to match our desired value of $\alpha$.

To carry out the correction, simply set your corrected $$\alpha_{corrected} = \frac{\alpha}{m}$$, where m is the number of tests you are performing. Rejecting the null hypothesis for each individual test at a significance level of $\leq \frac{\alpha}{m}$ keeps our FWER at $\leq \alpha$.

The proof for this inequality comes from Boole's inequality, which states that "for any finite countable set of events, the probability that at least one of the events happens is no greater than the sum of the probabilities of the individual events" (https://en.wikipedia.org/wiki/Boole%27s_inequality). In this case, $\alpha \leq m \cdot \alpha_{corrected}$.

__Exercise:__ In the case where we tested for an association between 20 colors of jelly beans and acne, what would our Bonferroni corrected $\alpha$ be?

<hide> 
$\alpha_{corrected} = \frac{0.05}{20} = 0.0025$
</hide>

An alternative way to perform the Bonferroni correction is not to adjust our significance level, but rather adjust our p-values themselves. To do this, simply multiply your p-values by the number of tests performed and use the uncorrected $\alpha$ as your significance level.

$$p_{adjusted} = m \cdot p$$

R has a function to adjust p-values through a variety of methods, including Bonferroni:
```{r}
adjusted_pvals <- p.adjust(c(0.05,0.0000001, 0.8), method = "bonferroni") 
print(adjusted_pvals)
```

Although the Bonferroni correction is relatively simple to implement, in practice it a very conservative correction (meaning that through employing it, you will likely be missing out on many true positives through overcorrecting for multiple tests, amplifying the number of FNs). This effect is especially powerful when you are carrying out a large number of tests or when the tests you are performing are not completely independent.

To explain why the Bonferroni correction is very conservative, we can explore what it does. The correction method sets the FWER to be __at most__ $\alpha$, which works well in cases when tests are truly independent. However, if there are any correlations/dependence (no matter how small), then the Bonferroni correction would be more stringent than strictly necessary. We rarely will run into situations in Biology where our tests are truly independent. For example, the genes expressed in your cells may exist in pathways, where the expression of one gene is correlated with the expression of other genes in the same pathway. Although individual jelly beans may have different flavoring compounds, they all share a large majority of their ingredients and are made in the same factory by largely the same machinery.

Because of this flaw in the Bonferroni correction, much effort has been devoted to developing methods that can correct for the number of FPs while not inflating the number of FNs.

### False Discovery Rates, q values, and the Benjamini-Hochberg Method

#### False discovery rates

In addition the the FWER, a different framework to conceptualize the number of incorrect conclusions you draw from performing a set of hypothesis tests is the False discovery rate (FDR).

When we set $\alpha = 0.05$ we are stating that about 5% of truly null tests will be called significant. An FDR = 0.05 means that __among the tests that are called significant (positive tests, or "discoveries"), about 5% will turn out to be null (false positive tests, or "false discoveries")__. In other words, $\alpha = 0.05$ means that if we performed 100 tests under the null, 5 of them would be false positives. FDR = 0.05 means that if we performed __X number of tests__ and found 100 significant tests, 5 of those significant results would actually be false positives. The first situation says that 5% of our tests will be false positives. The second situation tells us that 5% of our significant tests will be false positives.

$$FDR = \frac{FP}{FP+TP}$$

False discovery rate methods allow a certain number of false positives to slip through, but by doing so do a better job of finding more true positives (reducing the number of false negative tests).

#### q-values

Q-values are the name given to the adjusted p-values found using an optimised FDR approach. The FDR approach uses characteristics of a p-value distribution to produce a list of q-values.

__Exercise:__ If we were to perform a bunch of tests in a universe where all the null hypotheses are true and calculate their p-values, what would the distribution of p-values look like?

<hide> The distribution should look like a uniform distribution between 0 and 1. (Why? Hint: what is the definition of a p-value?) </hide> 

Observe the distributions of p-values collected from 2 different sets of 10000 tests. What is different between the two? Which do you think corresponds to a set of tests where the null hypothesis is true for all the tests?

```{r echo=FALSE}
set1 <- runif(10000)
set2p <- rexp(500, rate = 2)/10

hist(set1, breaks = seq(from = 0, to = 1, by = 0.05), main = "Set 1 p-values")
hist(c(set1[1:9500], set2p), breaks = seq(from = 0, to = 1, by = 0.05), main = "Set 2 p-values")
```

If there are no true positives in a set of tests, you will expect to see a distribution more like the first distribution, but if there are true positives in a set of tests you will expect to see a distribution more like the second. So, even if there are no true positives in the experiment, we still expect, by chance, to get p-values < 0.05. These are all false positives. However, even in an experiment with true positives, we are still unsure if a p-value < 0.05 represents a true positive or a false positive. This is because in the second set, the majority of tests are consistent with the null hypothesis, but there are a minority which are consistent with true positives. The resulting histogram is the sum of two histograms (one representing all the null tests, and one representing all the true positive tests). See below for a graphical depiction of this fact. Therefore, in the composite plot the true positives are mixed in with the false positives at the left side of the distribution! The q-value approach tries to find the height of the histogram where the p-value distribution flattens out on the right (where the majority of tests are drawn from the null distribution) and uses this information to determine what proportion of the values on the left side of the distribution are false positives vs true positives, giving us FDR adjusted p-values (q-values). 

```{r echo=FALSE}
hist(set1[1:9500], breaks = seq(from = 0, to = 1, by = 0.05), main = "Histogram of Null tests in Set 2")
hist(set2p, breaks = seq(from = 0, to = 1, by = 0.05), main = "Histogram of True Positive tests in Set 2")

hist(c(set1[1:9500], set2p), breaks = seq(from = 0, to = 1, by = 0.05), main = "Histogram of all tests in Set2")
abline(h = 460, col = "red")
```

A q-value threshold of 0.05 means that 0.05 of significant results will result in false positives. Another way to think about this is if we arrange all the q values for our set of tests from smallest to largest and pick one (for example a q-value of 0.10), then if we were to reject all hypotheses with a q-value less than this number in our set, we would expect 10% of them to be false positives. The most popular method of calculating q-values is known as the Benjamini-Hochberg method.

#### Benjamini-Hochberg Method

Before going into the mathematical details of the BH method, let us return to the p-value distribution we described above. 

```{r echo=FALSE}
hist(c(set1[1:9500], set2p), breaks = seq(from = 0, to = 1, by = 0.05), main = "Histogram of all tests in Set2")
abline(h = 460, col = "red")
```

Given that we know that the right side of the distribution is largely made up by null tests (which are uniformly flattly distributed between 0 and 1), and the true positive tests are concentrated around p-values of 0, how would you try to find a set of tests that would give you the highest proportion of true positives?

<hide> One thing we could do is take advantage of the red line we've drawn, which can be thought of as separating our two distributions apart. Notice how the number of true positive tests in each histogram bin is the number of tests in each bin that rise above the red line. In this case, we could be fairly confident in knowing that if we took the tests with the 350 lowest p-values, we would have a set that was enriched for true positives (since there are ~350 tests above the red line in the last bin). We can imagine that if we were to draw a circle around the p-values in the distribution that we wanted to keep as significant, we could determine what number of p-values to circle in order to have an FDR of approximately 0.05. </hide>

The BH method does something similar to the quick and dirty method we employed above, but instead of reporting p-values, it adjusts them to assign q-values to each test instead.

#### Actually performing the method:

The mathematics of employing the BH method are actually very simple. Here are the steps assuming you have a set of p-values from a set of hypothesis tests.

1. Order the p-values from smallest to largest and rank them from 1 (smallest) to m (largest)
2. Adjust the p-values using this equation:
$$q_i = min\{\frac{p_i m}{i}, q_{i+1}\}$$ 

where $q_i, p_i$ represents the q-value or p-value for the test ranked $i$, m represents the total number of tests in your set, and $q_{i+1}$ is the q-value for the next largest rank test.

And that's it! You would then interpret the q-values as we did above: "If we arrange all the q values for our set of tests from smallest to largest and pick one (for example a q-value of 0.10), then if we were to reject all hypotheses with a q-value less than this number in our set, we would expect 10% of them to be false positives."

Taking a closer look at the q-value equation, we can figure out what each part means. 

The numerator ($p_i m$) represents the expected number of FP if you accept all the tests with p-values of $p_i$ or smaller. Why is this the case? <hide> Recall that $p_i$ is the type 1 error rate, and m is the total number of results in your experiment. Just as rejecting all tests with a p-value of 0.05 or less means that 5% of our your tests will be false positives, the same can be said for any p-value. </hide> 

The denominator ($i$) represents the number of hypotheses you will reject at a threshold equal to $q_i$. Why is this? <hide> In this test, by rejecting the test corresponding to $q_i$, you will be also rejecting all tests with q-values less than $q_i$. This is equal to the rank number $i$. Therefore, the equation is  the expected number of FP based on the p-value divided by the total number of positives declared at that same p-value threshold. Recall that $FDR = \frac{FP}{FP+TP}$ </hide>. 

Finally, why do we take the minimum value between $\frac{p_i m}{i}$ and $q_{i+1}$? <hide> This allows us to maintain the same rank order of our p-values and our q-values. If you paid attention to the previous explanations of the other terms, you could have realized that I switched from discussing rejecting __p-values__ less than a certain threshold to talking about rejecting __q-values__ less than a certain threshold. By maintaining the rank order of our tests (making it so the adjusted p-value or q-value cannot be larger than that of higher rank order tests), we can make that switch. </hide>

#### Let's walk through an example on the board:

Suppose we have p-values for 10 tests: {0.01, 0.11, 0.21, 0.31, 0.41, 0.51, 0.61, 0.71, 0.81, 0.91}. We want to perform a Benjamini-Hochberg procedure on the p-values to obtain q-values.

<hide> {0.1, 0.55, 0.70, 0.77, 0.82, 0.85, 0.87, 0.89, 0.90, 0.91} </hide>



### Final thoughts

Methods that control for FWER (i.e. Bonferroni correction) maximize the purity of the discoveries you make (you can be more confident that the tests you call positive are indeed true positives rather than false postiives) at the cost of the completeness of those discoveries (you miss many true positives by commiting more type 2 errors). These types of corrections are most appropriate in situations where the costs of a false positive are very high.

Methods that control for FDR (i.e. Benjamini-Hochberg) relax the constraint on the purity of the discoveries you make (you still control the number of false positives you expect to have) in order to increase the completeness of your discoveries substantially. These types of corrections are most appropriate when you expect to have several true positives and can afford to have some false positives.

### Exercise:

Write a function that takes as input a vector of p-values and performs a Benjamini-Hochberg procedure to return a vector of q-values. Plot histograms of the vector of p-values in Set3 (found below) before and after correction.

```{r}
set.seed(1234)
set3 <- c(runif(10000), rexp(500, rate = 2)/10)
```

