---
title: "Multiple Testing Correction"
author: "Anthony Hung"
date: "2019-04-24"
output: 
  workflowr::wflow_html:
    code_folding: show
editor_options:
  chunk_output_type: console
  header-includes:
     - \usepackage{tabularx}
     - \usepackage{tcolorbox}
---

<style>
hide {
  background-color: #d6d6d6;
  color: #d6d6d6;
}
hide:hover {
  background-color: white;
  color: black;
}
</style>

## Introduction

Multiple testing describes situations where many hypotheses are simultaneously investigated from a given dataset. Correct treatment of statistics when working with multiple hypotheses is paramount, as mistakes can easily lead to false interpretations of results and many false positives. Our objectives today are to review the framework behind hypothesis testing in single hypotheses, why this framework falls apart in multiple testing, and different methods that have been proposed to correct for multiple testing.

## Hypothesis testing

The basic idea in hypothesis testing is to use data or observations to choose between two possible realities: a null hypothesis or an alternative hypothesis. 

## The issue of multiple testing

As many scientific fields enter an age of "Big Data," where the ability to collect and work with data from a large number of measurements gives rise to the ability to test many hypotheses at the same time. However, as scientistists tests many more hypotheses, the standard view of hypothesis testing falls apart. 

To illustrate this, consider the xkcd comic (https://xkcd.com/882/). Obviously, something is not right with the conclusions of the study, since we all have an intuition that green jelly beans do not have any true association with skin conditions. To better understand why mutliple testing can easily lead to false positive associations unless adequately treated, let us walk through the calculations for the probability of making a Type 1 error given the number of tests you are performing.

### Case 1: Performing 1 test

Let us say we are performing the study in the comic and testing for a link between purple jelly beans and acne at a significance level $\alpha = 0.05$. What is the probability that we make a type 1 error?

<hide> As $\alpha$ is equal to our type 1 error rate (the probability of rejecting the null hypothesis given the null hypothesis is true), we know that the probability is equal to 0.05.</hide>

### Case 2: Performing 20 tests

Now, let us test for an association between 20 different colors of jelly beans and acne at a significance level of $\alpha = 0.05$ for each individual test. What is the probability that we make at least one type 1 error now?

<hide> 
Here, we are interested in finding the P(making a type 1 error), which is the same as 1 - P(NOT making a type 1 error). The P(NOT making a type 1 error) for each of the individual tests is equal to $1- \alpha = 0.95$. If we assume that each of the separate tests is independent, then our probability of making at least one type 1 error amongst our 20 tests is $1-(1-0.05)^{20} = 0.6415$.
</hide>

This makes sense if we go back to the definition of a p-value. The p-value measures the probability of seeing an observation as extreme or more extreme than your data given that the data were drawn from a null distribution. If we have a p-value of 0.05, that is equivalent to saying that 5% of the time, in a universe where the null hypothesis is true, we would see data as extreme or more extreme that our data. (See drawing on chalkboard). If we perform 100 tests in a universe where the null hypothesis is true, then we expect 5 of those tests to have p-values less than 0.05! Clearly, we need to take into account the number of tests when setting our significance threshold. 

In the above two examples, what we calculated was the probability of making __at least one type 1 error__ in all the independent tests we were performing. This is otherwise known as the __Family-Wise Error Rate (FWER)__.


## Correcting for multiple tests:

### Bonferroni Correction

The Bonferroni correction is the simplest method for correcting for multiple testing, and it can be applied to cases where you are performing multiple dependent or independent tests. The theoretical basis behind this correction is to attempt to adjust our FWER to match our desired value of $\alpha$.

To carry out the correction, simply set your corrected $$\alpha_{corrected} = \frac{\alpha}{m}$$, where m is the number of tests you are performing. Rejecting the null hypothesis for each individual test at a significance level of $\leq \frac{\alpha}{m}$ keeps our FWER at $\leq \alpha$.

The proof for this inequality comes from Boole's inequality, which states that "for any finite countable set of events, the probability that at least one of the events happens is no greater than the sum of the probabilities of the individual events" (https://en.wikipedia.org/wiki/Boole%27s_inequality). In this case, $\alpha \leq m \cdot \alpha_{corrected}$.

__Exercise:__ In the case where we tested for an association between 20 colors of jelly beans and acne, what would our Bonferroni corrected $\alpha$ be?

<hide> 
$\alpha_{corrected} = \frac{0.05}{20} = 0.0025$
</hide>

An alternative way to perform the Bonferroni correction is not to adjust our significance level, but rather adjust our p-values themselves. To do this, simply multiply your p-values by the number of tests performed and use the uncorrected $\alpha$ as your significance level.

$$p_{adjusted} = m \cdot p$$

R has a function to adjust p-values through a variety of methods, including Bonferroni:
```{r}
adjusted_pvals <- p.adjust(c(0.05,0.0000001, 0.8), method = "bonferroni") 
print(adjusted_pvals)
```

Although the Bonferroni correction is relatively simple to implement, in practice it a very conservative correction (meaning that through employing it, you will likely be missing out on many true positives through overcorrecting for multiple tests, amplifying the number of FNs). This effect is especially powerful when you are carrying out a large number of tests or when the tests you are performing are not completely independent. 

Because of this flaw in the Bonferroni correction, much effort has been devoted to developing methods that can correct for the number of FPs while not inflating the number of FNs.

### Holm's Procedure

Holm's procedure also corrects for the FWER, but is uniformly more powerful than the Bonferroni correction (it gives you fewer FNs for any range of p-value than if you were to use the Bonferroni correction). 

The steps for carrying out the procedure given you are performing m hypotheses:

* Order your p-values from smallest to largest ($P_1, P_2, P_3, P_4$) corresponding to null hypotheses ($H_1, H_2, H_3, H_4$).

* For each rank k starting with 1 (corresponding to your smallest p-value), calculate an $\alpha_{adjustedk}$ such that:

$$\alpha_{adjustedk}=\frac{\alpha}{m+ 1 -k}$$

* Compare each p-value with its corresponding adjusted significance level, starting from the smallest p-value. Keep comparing until you find a p-value that is __greater than__ its adjusted significance level.

* Reject the hypotheses corresponding to the ranked p-values up to and __not including__ the first p-value that is greater than its adjusted significance level.


__Exercise:__ That probably was a bit confusing, so let us walk through an example:

Suppose we have 4 p-values: {0.5, 0.01, 0.001, 0.3}. We want to perform a Holm procedure correction to get our FWER to 0.05

* Order the p-values from smallest to largest

<hide> {0.001, 0.01, 0.3, 0.5} </hide>

* Calculate the adjusted $\alpha$ for the first rank test.

<hide> $\alpha_{adjusted1}=\frac{\alpha}{m+ 1 -1} = \frac{0.05}{4+ 1 -1} = 0.0125$</hide>

* Compare our adjusted $\alpha$ for the first rank test to its p-value.

<hide> $0.0125 > 0.001$. Reject the first rank hypothesis (the hypothesis with the lowest p-value of the 4). </hide>

* Now, calculate the adjusted $\alpha$ for the second rank test.

<hide> $\alpha_{adjusted2}=\frac{0.05}{4+ 1 -2} = 0.01666667$</hide>

* Compare our adjusted $\alpha$ for the second rank test to its p-value.

<hide> $0.01666667 > 0.01$. Reject the second rank hypothesis (the hypothesis with the second lowest p-value of the 4). </hide>

* Now, calculate the adjusted $\alpha$ for the third rank test.

<hide> $\alpha_{adjusted3}=\frac{0.05}{4+ 1 -3} = 0.025$</hide>

* Compare our adjusted $\alpha$ for the second rank test to its p-value.

<hide> $0.025 < 0.3$. Do not reject the third rank hypothesis (or any other hypothesis with higher rank). </hide>

The rationale behind the Holm procedure is that it still corrects for the FWER like the Bonferroni correction, but whereas the Bonferroni correction applied the same $\alpha_{corrected}$ to all tests no matter how large their p-values were, the Holm procedure applies the most stringent threshold to the smallest p-value ($\alpha_{corrected1}= \frac{\alpha}{m}$) and applies the least stringent threshold to the largest p-value ($\alpha_{correctedm}= \frac{\alpha}{1}$). In other words, larger p-values experience less of a correction than smaller p-values, meaning you are more likely to reject their hypotheses compared to if you applied the same stringent significance level uniformly to all tests.

### False Discovery Rates, q values, and the Benjamini-Hochberg Method

#### False discovery rates

In addition the the FWER, a different framework to conceptualize the number of incorrect conclusions you draw from performing a set of hypothesis tests is the False discovery rate (FDR).

When we set $\alpha = 0.05$ we are stating that about 5% of truly null tests will be called significant. An FDR = 0.05 means that __among the tests that are called significant, about 5% will turn out to be null (false discoveries)__. In other words, $\alpha = 0.05$ means that if we performed 100 tests under the null, 5 of them would be false positives. FDR = 0.05 means that if we performed __X number of tests__ and found 100 significant tests, 5 of those significant results would actually be false positives. The first situation says that 5% of our tests will be false positives. The second situation tells us that 5% of our significant tests will be false positives.

$$FDR = \frac{FP}{FP+TP}$$

#### q-values

Q-values are the name given to the adjusted p-values found using an optimised FDR approach. The FDR approach uses characteristics of a p-value distribution to produce a list of q-values.

__Exercise:__ If we were to perform a bunch of tests in a universe where all the null hypotheses are true and calculate their p-values, what would the distribution of p-values look like?

<hide> The distribution should look like a uniform distribution between 0 and 1. (Why? Hint: what is the definition of a p-value?) </hide> 

Observe the distributions of p-values collected from 2 different sets of 10000 tests. What is different between the two? Which do you think corresponds to a set of tests where the null hypothesis is true for all the tests?

```{r echo=FALSE}
set1 <- runif(10000)
set2p <- rexp(500, rate = 2)/10

hist(set1, breaks = seq(from = 0, to = 1, by = 0.05), main = "Set 1 p-values")
hist(c(set1[1:9500], set2p), breaks = seq(from = 0, to = 1, by = 0.05), main = "Set 2 p-values")
```

If there are no true positives in a set of tests, you will expect to see a distribution more like the first distribution, but if there are true positives in a set of tests you will expect to see a distribution more like the second. So, even if there are no true positives in the experiment, we still expect, by chance, to get p-values < 0.05. These are all false positives. However, even in an experiment with true positives, we are still unsure if a p-value < 0.05 represents a true positive or a false positive. This is because in the second set, the majority of tests are consistent with the null hypothesis, but there are a minority which are consistent with true positives. The resulting histogram is the sum of two histograms (one representing all the null tests, and one representing all the true positive tests). See below for a graphical depiction of this fact. Therefore, in the composite plot the true positives are mixed in with the false positives at the left side of the distribution! The q-value approach tries to find the height of the histogram where the p-value distribution flattens out on the right (where the majority of tests are drawn from the null distribution) and uses this information to determine what proportion of the values on the left side of the distribution are false positives vs true positives, giving us FDR adjusted p-values (q-values). 

```{r echo=FALSE}
hist(set1[1:9500], breaks = seq(from = 0, to = 1, by = 0.05), main = "Histogram of Null tests in Set 2")
hist(set2p, breaks = seq(from = 0, to = 1, by = 0.05), main = "Histogram of True Positive tests in Set 2")

hist(c(set1[1:9500], set2p), breaks = seq(from = 0, to = 1, by = 0.05), main = "Histogram of all tests in Set2")
abline(h = 460, col = "red")
```

A q-value threshold of 0.05 means that 0.05 of significant results will result in false positives. Another way to think about this is if we arrange all the q values for our set of tests from smallest to largest and pick one (for example a q-value of 0.10), then if we were to reject all hypotheses with a q-value less than this number in our set, we would expect 10% of them to be false positives. The most popular method of calculating q-values is known as the Benjamini-Hochberg method.

#### Benjamini-Hochberg Method

Before going into the mathematical details of the BH method, let us return to the p-value distribution we described above. 

```{r echo=FALSE}
hist(c(set1[1:9500], set2p), breaks = seq(from = 0, to = 1, by = 0.05), main = "Histogram of all tests in Set2")
abline(h = 460, col = "red")
```

Given that we know that the right side of the distribution is largely made up by null tests (which are uniformly flattly distributed between 0 and 1), and the true positive tests are concentrated around p-values of 0, how would you try to find a set of tests that would give you the highest proportion of true positives?

<hide> One thing we could do is take advantage of the red line we've drawn, which can be thought of as separating our two distributions apart. Notice how the number of true positive tests in each histogram bin is the number of tests in each bin that rise above the red line. In this case, we could be fairly confident in knowing that if we took the tests with the 350 lowest p-values, we would have a set that was enriched for true positives (since there are ~350 tests above the red line in the last bin). We can imagine that if we were to draw a circle around the p-values in the distribution that we wanted to keep as significant, we could determine what number of p-values to circle in order to have an FDR of approximately 0.05. </hide>

The BH method does something similar to the quick and dirty method we employed above, but instead of reporting p-values, it adjusts them to assign q-values to each test instead.

#### Actually performing the method:

The mathematics of employing the BH method are actually very simple. Here are the steps assuming you have a set of p-values from a set of hypothesis tests.

1. Order the p-values from smallest to largest and rank them from 1 (smallest) to m (largest)
2. Adjust the p-values using this equation:
$$q_i = min\{\frac{p_i m}{i}, q_{i+1}\}$$ 

where $q_i, p_i$ represents the q-value or p-value for the test ranked $i$, m represents the total number of tests in your set, and $q_{i+1}$ is the q-value for the next largest rank test.

And that's it! You would then interpret the q-values as we did above: "If we arrange all the q values for our set of tests from smallest to largest and pick one (for example a q-value of 0.10), then if we were to reject all hypotheses with a q-value less than this number in our set, we would expect 10% of them to be false positives."

Taking a closer look at the q-value equation, we can figure out what each part means. 

The numerator ($p_i m$) represents the expected number of FP if you accept all the tests with p-values of $p_i$ or smaller. Why is this the case? <hide> Recall that $p_i$ is the type 1 error rate, and m is the total number of results in your experiment. Just as rejecting all tests with a p-value of 0.05 or less means that 5% of our your tests will be false positives, the same can be said for any p-value. </hide> 

The denominator ($i$) represents the number of hypotheses you will reject at a threshold equal to $q_i$. Why is this? <hide> In this test, by rejecting the test corresponding to $q_i$, you will be also rejecting all tests with q-values less than $q_i$. This is equal to the rank number $i$. Therefore, the equation is  the expected number of FP based on the p-value divided by the total number of positives declared at that same p-value threshold. Recall that $FDR = \frac{FP}{FP+TP}$ </hide>. 

Finally, why do we take the minimum value between $\frac{p_i m}{i}$ and $q_{i+1}$? <hide> This allows us to maintain the same rank order of our p-values and our q-values. If you paid attention to the previous explanations of the other terms, you could have realized that I switched from discussing rejecting __p-values__ less than a certain threshold to talking about rejecting __q-values__ less than a certain threshold. By maintaining the rank order of our tests (making it so the adjusted p-value or q-value cannot be larger than that of higher rank order tests), we can make that switch. </hide>

#### Let's walk through an example on the board:

Suppose we have p-values for 10 tests: {0.01, 0.11, 0.21, 0.31, 0.41, 0.51, 0.61, 0.71, 0.81, 0.91}. We want to perform a Benjamini-Hochberg procedure on the p-values to obtain q-values.

<hide> {0.1, 0.55, 0.70, 0.77, 0.82, 0.85, 0.87, 0.89, 0.90, 0.91} </hide>

### Exercise:

Write a function that takes as input a vector of p-values and performs a Benjamini-Hochberg procedure to return a vector of q-values. Plot histograms of the vector of p-values in Set3 (found below) before and after correction.

```{r}
set.seed(1234)
set3 <- c(runif(10000), rexp(500, rate = 2)/10)
```


