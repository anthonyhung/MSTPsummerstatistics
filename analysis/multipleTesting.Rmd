---
title: "Multiple Testing Correction"
author: "Anthony Hung"
date: "2019-04-24"
output: 
  workflowr::wflow_html:
    code_folding: show
editor_options:
  chunk_output_type: console
  header-includes:
     - \usepackage{tabularx}
     - \usepackage{tcolorbox}
---

<style>
hide {
  background-color: #d6d6d6;
  color: #d6d6d6;
}
hide:hover {
  background-color: white;
  color: black;
}
</style>

## Introduction

Multiple testing describes situations where many hypotheses are simultaneously investigated from a given dataset. Correct treatment of statistics when working with multiple hypotheses is paramount, as mistakes can easily lead to false interpretations of results and many false positives. Our objectives today are to review the framework behind hypothesis testing in single hypotheses, why this framework falls apart in multiple testing, and different methods that have been proposed to correct for multiple testing.

## Hypothesis testing

The basic idea in hypothesis testing is to use data or observations to choose between two possible realities: a null hypothesis or an alternative hypothesis. 

## The issue of multiple testing

As many scientific fields enter an age of "Big Data," where the ability to collect and work with data from a large number of measurements gives rise to the ability to test many hypotheses at the same time. However, as scientistists tests many more hypotheses, the standard view of hypothesis testing falls apart. 

To illustrate this, consider the xkcd comic (https://xkcd.com/882/). Obviously, something is not right with the conclusions of the study, since we all have an intuition that green jelly beans do not have any true association with skin conditions. To better understand why mutliple testing can easily lead to false positive associations unless adequately treated, let us walk through the calculations for the probability of making a Type 1 error given the number of tests you are performing.

### Case 1: Performing 1 test

Let us say we are performing the study in the comic and testing for a link between purple jelly beans and acne at a significance level $\alpha = 0.05$. What is the probability that we make a type 1 error?

<hide> As $\alpha$ is equal to our type 1 error rate (the probability of rejecting the null hypothesis given the null hypothesis is true), we know that the probability is equal to 0.05.</hide>

### Case 2: Performing 20 tests

Now, let us test for an association between 20 different colors of jelly beans and acne at a significance level of $\alpha = 0.05$ for each individual test. What is the probability that we make at least one type 1 error now?

<hide> 
Here, we are interested in finding the P(making a type 1 error), which is the same as 1 - P(NOT making a type 1 error). The P(NOT making a type 1 error) for each of the individual tests is equal to $1- \alpha = 0.95$. If we assume that each of the separate tests is independent, then our probability of making at least one type 1 error amongst our 20 tests is $1-(1-0.05)^{20} = 0.6415$.
</hide>

This makes sense if we go back to the definition of a p-value. The p-value measures the probability of seeing an observation as extreme or more extreme than your data given that the data were drawn from a null distribution. If we have a p-value of 0.05, that is equivalent to saying that 5% of the time, in a universe where the null hypothesis is true, we would see data as extreme or more extreme that our data. (See drawing on chalkboard). If we perform 100 tests in a universe where the null hypothesis is true, then we expect 5 of those tests to have p-values less than 0.05! Clearly, we need to take into account the number of tests when setting our significance threshold. 

In the above two examples, what we calculated was the probability of making __at least one type 1 error__ in all the independent tests we were performing. This is otherwise known as the __Family-Wise Error Rate (FWER)__.


## Correcting for multiple tests:

### Bonferroni Correction

The Bonferroni correction is the simplest method for correcting for multiple testing, and it can be applied to cases where you are performing multiple dependent or independent tests. The theoretical basis behind this correction is to attempt to adjust our FWER to match our desired value of $\alpha$.

To carry out the correction, simply set your corrected $$\alpha_{corrected} = \frac{\alpha}{m}$$, where m is the number of tests you are performing. Rejecting the null hypothesis for each individual test at a significance level of $\leq \frac{\alpha}{m}$ keeps our FWER at $\leq \alpha$.

The proof for this inequality comes from Boole's inequality, which states that "for any finite countable set of events, the probability that at least one of the events happens is no greater than the sum of the probabilities of the individual events" (https://en.wikipedia.org/wiki/Boole%27s_inequality). In this case, $\alpha \leq m \cdot \alpha_{corrected}$.

__Exercise:__ In the case where we tested for an association between 20 colors of jelly beans and acne, what would our Bonferroni corrected $\alpha$ be?

<hide> 
$\alpha_{corrected} = \frac{0.05}{20} = 0.0025$
</hide>

An alternative way to perform the Bonferroni correction is not to adjust our significance level, but rather adjust our p-values themselves. To do this, simply multiply your p-values by the number of tests performed and use the uncorrected $\alpha$ as your significance level.

$$p_{adjusted} = m \cdot p$$

R has a function to adjust p-values through a variety of methods, including Bonferroni:
```{r}
adjusted_pvals <- p.adjust(c(0.05,0.0000001, 0.8), method = "bonferroni") 
print(adjusted_pvals)
```

Although the Bonferroni correction is relatively simple to implement, in practice it a very conservative correction (meaning that through employing it, you will likely be missing out on many true positives through overcorrecting for multiple tests, amplifying the number of FNs). This effect is especially powerful when you are carrying out a large number of tests or when the tests you are performing are not completely independent. 

Because of this flaw in the Bonferroni correction, much effort has been devoted to developing methods that can correct for the number of FPs while not inflating the number of FNs.

### Holm's Procedure

Holm's procedure also corrects for the FWER, but is uniformly more powerful than the Bonferroni correction (it gives you fewer FNs for any range of p-value than if you were to use the Bonferroni correction). 

The steps for carrying out the procedure given you are performing m hypotheses:

* Order your p-values from smallest to largest ($P_1, P_2, P_3, P_4$) corresponding to null hypotheses ($H_1, H_2, H_3, H_4$).

* For each rank k starting with 1 (corresponding to your smallest p-value), calculate an $\alpha_{adjustedk}$ such that:

$$\alpha_{adjustedk}=\frac{\alpha}{m+ 1 -k}$$

* Compare each p-value with its corresponding adjusted significance level, starting from the smallest p-value. Keep comparing until you find a p-value that is __greater than__ its adjusted significance level.

* Reject the hypotheses corresponding to the ranked p-values up to and __not including__ the first p-value that is greater than its adjusted significance level.


__Exercise:__ That probably was a bit confusing, so let us walk through an example:

Suppose we have 4 p-values: {0.5, 0.01, 0.001, 0.3}. We want to perform a Holm procedure correction to get our FWER to 0.05

* Order the p-values from smallest to largest

<hide> {0.001, 0.01, 0.3, 0.5} </hide>

* Calculate the adjusted $\alpha$ for the first rank test.

<hide> $\alpha_{adjusted1}=\frac{\alpha}{m+ 1 -1} = \frac{0.05}{4+ 1 -1} = 0.0125$</hide>

* Compare our adjusted $\alpha$ for the first rank test to its p-value.

<hide> $0.0125 > 0.001$. Reject the first rank hypothesis (the hypothesis with the lowest p-value of the 4). </hide>

* Now, calculate the adjusted $\alpha$ for the second rank test.

<hide> $\alpha_{adjusted2}=\frac{0.05}{4+ 1 -2} = 0.01666667$</hide>

* Compare our adjusted $\alpha$ for the second rank test to its p-value.

<hide> $0.01666667 > 0.01$. Reject the second rank hypothesis (the hypothesis with the second lowest p-value of the 4). </hide>

* Now, calculate the adjusted $\alpha$ for the third rank test.

<hide> $\alpha_{adjusted3}=\frac{0.05}{4+ 1 -3} = 0.025$</hide>

* Compare our adjusted $\alpha$ for the second rank test to its p-value.

<hide> $0.025 < 0.3$. Do not reject the third rank hypothesis (or any other hypothesis with higher rank). </hide>

The rationale behind the Holm procedure is that it still corrects for the FWER like the Bonferroni correction, but whereas the Bonferroni correction applied the same $\alpha_{corrected}$ to all tests no matter how large their p-values were, the Holm procedure applies the most stringent threshold to the smallest p-value ($\alpha_{corrected1}= \frac{\alpha}{m}$) and applies the least stringent threshold to the largest p-value ($\alpha_{correctedm}= \frac{\alpha}{1}$). In other words, larger p-values experience less of a correction than smaller p-values, meaning you are more likely to reject their hypotheses compared to if you applied the same stringent significance level uniformly to all tests.

### False Discovery Rates, q values, and the Benjamini-Hochberg Method

#### False discovery rates

In addition the the FWER, a different framework to conceptualize the number of incorrect conclusions you draw from performing a set of hypothesis tests is the False discovery rate (FDR).

When we set $\alpha = 0.05$ we are stating that about 5% of truly null tests will be called significant. An FDR = 0.05 means that __among the tests that are called significant, about 5% will turn out to be null (false discoveries)__. In other words, $\alpha = 0.05$ means that if we performed 100 tests under the null, 5 of them would be false positives. FDR = 0.05 means that if we performed __X number of tests__ and found 100 significant tests, 5 of those significant results would actually be false positives. The first situation says that 5% of our tests will be false positives. The second situation tells us that 5% of our significant tests will be false positives.

$$FDR = \frac{FP}{FP+TP}$$

#### q-values

Q-values are the name given to the adjusted p-values found using an optimised FDR approach. The FDR approach uses characteristics of a p-value distribution to produce a list of q-values.

__Exercise:__ If we were to perform a bunch of tests in a universe where all the null hypotheses are true and calculate their p-values, what would the distribution of p-values look like?

<hide> The distribution should look like a uniform distribution between 0 and 1. (Why? Hint: what is the definition of a p-value?) </hide> 

Observe the distributions of p-values collected from 2 different sets of 10000 tests. What is different between the two? Which do you think corresponds to a set of tests where the null hypothesis is true for all the tests?

```{r echo=FALSE}
set1 <- runif(10000)
set2p <- rexp(500)/10

hist(set1, breaks = seq(from = 0, to = 1, by = 0.05), main = "Set 1 p-values")
hist(c(set1[1:9500], set2p), breaks = seq(from = 0, to = 1, by = 0.05), main = "Set 2 p-values")
```

If there are no true positives in a set of tests, you will expect to see a distribution more like the first distribution, but if there are true positives in a set of tests you will expect to see a distribution more like the second. So, even if there are no true positives in the experiment, we still expect, by chance, to get p-values < 0.05. These are all false positives. However, even in an experiment with true positives, we are still unsure if a p-value < 0.05 represents a true positive or a false positive. This is because in the second set, the majority of tests are consistent with the null hypothesis, but there are a minority which are consistent with true positives. The resulting histogram is the sum of two histograms (one representing all the null tests, and one representing all the true positive tests). See below for a graphical depiction of this fact. Therefore, in the composite plot the true positives are mixed in with the false positives at the left side of the distribution! The q-value approach tries to find the height of the histogram where the p-value distribution flattens out on the right (where the majority of tests are drawn from the null distribution) and uses this information to determine what proportion of the values on the left side of the distribution are false positives vs true positives, giving us FDR adjusted p-values (q-values). 

```{r echo=FALSE}
hist(set1[1:9500], breaks = seq(from = 0, to = 1, by = 0.05), main = "Histogram of Null tests in Set 2")
hist(set2p, breaks = seq(from = 0, to = 1, by = 0.05), main = "Histogram of True Positive tests in Set 2")

hist(c(set1[1:9500], set2p), breaks = seq(from = 0, to = 1, by = 0.05), main = "Histogram of all tests in Set2")
abline(h = 460, col = "red")
```

q-values are defined as such:

$$q_i(p_i) = min_{t \ge p_i}FDR(t)$$

Q-value table screenshot
To interpret the q-values, you need to look at the ordered list of q-values. There are 3516 compounds in this experiment. If we take unknown compound 1723 as an example, we see that it has a p-value of 0.0101 and a q-value of 0.0172. Recall that a p-value of 0.0101 implies a 1.01% chance of false positives, and so with 3516 compounds, we expect about 36 false positives, i.e. 3516 × 0.0101 = 35.51. In this experiment, there are 800 compounds with a value of 0.0101 or less, and so 36 of these will be false positives.

On the other hand, the q-value is 0.0172, which means we should expect 1.72% of all the compounds with q-value less than this to be false positives. This is a much better situation. We know that 800 compounds have a q-value of 0.0172 or less and so we should expect 800 × 0.0172 = 13.76 false positives rather than the predicted 36. Just to reiterate, false positives according to p-values take all 3516 values into account when determining how many false positives we should expect to see while q-values take into account only those tests with q-values less than the threshold we choose. Of course, it is not always the case that q-values will result in less false positives, but what we can say is that they give a far more accurate indication of the level of false positives for a given cut-off value.

When doing lots of tests, as in a metabolomics experiment, it is more intuitive to interpret p and q values by looking at the entire list of values in this way rather that looking at each one independently. In this way, a threshold of 0.05 has meaning across the entire experiment. When deciding on a cut-off or threshold value, you should do this from the point of view of how many false positives will this result in, rather than just randomly picking a p- or q-value of 0.05 and saying that every compound with p-value less this is significant.

#### Benjamini-Hochberg Method


### Exercise:

